{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351659dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!sudo huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef816c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73eb857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498af470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_id = \"mrm8488/llama-2-coder-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1c1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d2d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce8d91a197b4039acce3c8ab88beb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abefc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(instruction):\n",
    "  system = \"Translator:\"\n",
    "  instruction = \"### Instruction: \" + instruction\n",
    "  return system + \"\\n\" + instruction + \"\\n\\n\" + \"### Solution:\" + \"\\n\"\n",
    "\n",
    "def generate(\n",
    "        instruction,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=4,\n",
    "        **kwargs,\n",
    "):\n",
    "    prompt = create_prompt(instruction)\n",
    "    print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.split(\"### Solution:\")[1].lstrip(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a002d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "You are a sentence mapping expert. You can transform any sentence in a given format to another format. \n",
    "The sentence mapping should do by a three level tasks. All the tasks has its own rules to follow and objectives to consider.\n",
    "You must make sure that you are doing all the tasks in order by following the given rules.\n",
    "The input sentence here after known as actual sentence.\n",
    "\n",
    "Task 1:\n",
    "Extract dimensions and measures from the  actual sentence\n",
    "\n",
    "Rules for task 1:\n",
    "Rule 1 : Dimensions and measures are domain dependent and must identified based on the given metadata\n",
    "\"metadata\": <Metadata>\n",
    "Examples for task 1:\n",
    "sentence : which country and state has the highest sales and lowest profit\n",
    "Output: {dimension:[country,state],measure:[sales,profit]}\n",
    "\n",
    "Task 2:\n",
    "Generate possible questions based on the Extracted dimensions and measures from task 1.\n",
    "\n",
    "Rules for task 2:\n",
    "Rule 3 : Dont deviate from the template structure\n",
    "Rule 4 : Consider the number of dimensions and measures in the question template given with the\n",
    "number of dimensions and measures which were extracted from the actual sentence \n",
    "Rule 5 :Generate sentences if the dimensions and measure count in the template\n",
    "are equal to the count  of extracted dimensions and measures from the actual sentence.\n",
    "Rule 6 : Ensure Rule 4 and Rule 5 strictly \n",
    "\n",
    "Examples for task 2:\n",
    "input : {dimension:[country,state],measure:[sales,profit]}\n",
    "Templates: {\n",
    "            How many {dimension1} are there: {'dimension': 1},\n",
    "            How many {dimension1} across {finite_interval1}: {'dimension': 1, 'finite_interval': 1},\n",
    "            How many {dimension1} where {measure1} is {measure_filter1}: {'dimension': 1, 'measure': 1, 'measure_filter': 1},\n",
    "            How many {dimension1} across {finite_interval1} where {measure1} is {measure_filter1}: {'dimension': 1, 'finite_interval': 1, 'measure': 1, 'measure_filter': 1},\n",
    "            How many {dimension1} contributing to growth of {measure1} across {finite_interval1}: {'dimension': 1, 'finite_interval': 1, 'measure': 1}\n",
    "            How many {dimension1} in the {dimension2 } where {measure1} is half of {measure2}: {'dimension': 2, 'measure': 2},\n",
    "        }\n",
    "Generated sentences : [\n",
    "            How many country in the state where sales is half of profit,\n",
    "            How many state in the country where sales is half of profit,\n",
    "            How many country in the state where profit is half of sales,\n",
    "            How many state in the country where profit is half of sales\n",
    "        ]\n",
    "Task 3:\n",
    "\n",
    "Find the most similar sentence to the actual sentence from the Generated sentence list\n",
    "\n",
    "Rules for task 3:\n",
    "Rule 1 : Find the semantic similarity between the actual question and the generated sentences\n",
    "Rule 2 : Rank them in the descending order\n",
    "Rule 3 : Return the first ranked sentence as response\n",
    "\n",
    "Examples for task 3:\n",
    "Generated sentence list:[\n",
    "    How many country in the state where sales is half of profit,\n",
    "    How many state in the country where sales is half of profit,\n",
    "    How many country in the state where profit is half of sales,\n",
    "    How many state in the country where profit is half of sales\n",
    "      ]\n",
    "Actual sentence : How many state in the country where profit is double of sales\n",
    "Response:{\"formatted sentence\": \"How many state in the country where sales is half of profit\",\"status code\":200}\n",
    "\n",
    "Now you have the whole knowledge about the process. Now transform the given sentence in the updated format.\n",
    "Return only the formatted sentence with status code .\n",
    "\n",
    "Strictly saying, I dont need the result of each task. Only show the last tasks output as explained above.\n",
    "Dont show any intermediate answers.\n",
    "\n",
    "Sentence: what did the sales fluctuate in jan 2023\n",
    "\"\"\"\n",
    "\n",
    "print(generate(instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4f750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a coding assistant that will help the user to resolve the following instruction:\n",
      "### Instruction: \n",
      "What is this SAS program doing\n",
      "data abcd;\n",
      "  infile \"abcd\";\n",
      "  input;\n",
      "run;\n",
      "\n",
      "data abcd_profile;\n",
      "  set abcd;\n",
      "  output;\n",
      "run;\n",
      "\n",
      "data abcd_profile;\n",
      "  set abcd_profile;\n",
      "  output;\n",
      "run;\n",
      "\n",
      "data abcd_profile;\n",
      "  set abcd_profile;\n",
      "  output;\n",
      "run;\n",
      "\n",
      "data abcd_profile;\n",
      "  set abcd_profile;\n",
      "  output;\n",
      "run;\n",
      "\n",
      "\n",
      "### Solution:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = \"\"\"\n",
    "What is this SAS program doing\n",
    "data abcd;\n",
    "  infile \"abcd\";\n",
    "  input;\n",
    "run;\n",
    "\n",
    "data abcd_profile;\n",
    "  set abcd;\n",
    "  output;\n",
    "run;\n",
    "\n",
    "data abcd_profile;\n",
    "  set abcd_profile;\n",
    "  output;\n",
    "run;\n",
    "\n",
    "data abcd_profile;\n",
    "  set abcd_profile;\n",
    "  output;\n",
    "run;\n",
    "\n",
    "data abcd_profile;\n",
    "  set abcd_profile;\n",
    "  output;\n",
    "run;\n",
    "\"\"\"\n",
    "\n",
    "print(generate(instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81570d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
