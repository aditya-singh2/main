{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091516f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo -H pip install -q transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall autoawq -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc12a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4a3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !sudo -H pip install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.7/autoawq-0.1.7+cu118-cp38-cp38-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779984ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29fd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603676d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffccfab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = '/data/mistral/query-to-mql/exp-10/nov-20/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c42249",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_path = '/llmmodels/quantized_model/awq_mistral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e46e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89d01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = {\"zero_point\":True,\n",
    "               \"q_group_size\": 128,\n",
    "               \"w_bit\": 4,\n",
    "               \"version\": \"GEMM\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f853010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cca17da3c34fb38c633751336968bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd7da91af68417e95b99f533793dd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model\n",
    "# model = AutoAWQForCausalLM.from_pretrained(model_path, **{\"low_cpu_mem_usage\":True})\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e50621ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ffa0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8fa7742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636aa93d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pip_packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "AWQ:  84%|████████▍ | 27/32 [34:44<06:17, 75.54s/it]"
     ]
    }
   ],
   "source": [
    "#Quantize\n",
    "# dont run it again\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_quantized = AutoModelForCausalLM.from_pretrained(\n",
    "#     model,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     quantization_config=quant_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb126257",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac71427",
   "metadata": {},
   "source": [
    "### Inference on quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "quant_path = '/data/quantization-trials/quant-model'\n",
    "model_path = '/data/quantization-trials/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7b2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = quant_path\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "def predict_from_quant(user_query):\n",
    "    _inputs = tokenizer.encode(user_query, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = quant_model.generate(input_ids=_inputs, max_length= 1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using quant model\n",
    "start = time.time()\n",
    "output1 = predict_from_quant(\"what is art\")\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f1d27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fee82",
   "metadata": {},
   "source": [
    "### Inference on un-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf676e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data/quantization-trials/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model_path\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "def predict_from_normal(user_query):\n",
    "    _inputs = tokenizer.encode(user_query, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(input_ids=_inputs, max_length= 1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b00db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using unquant model\n",
    "import time\n",
    "start = time.time()\n",
    "output2 = predict_from_normal(\"what is art\")\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b6f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0949f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139bb23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quant_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
