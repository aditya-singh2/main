{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1ce2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076fbe7e",
   "metadata": {},
   "source": [
    "# Download and Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "# pipe.save_pretrained(\"/data/finbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5ef2c",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45c6955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"/data/finbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b87b8d",
   "metadata": {},
   "source": [
    "# Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edbd2088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.8971117734909058}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"this stock is going to fall, I wont buy it\"#, \"this has some potential but I am still not confident\"\n",
    "pipe(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc17593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.040109872817993164"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"before model start\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(sentences)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a957e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.34.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5453fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "\n",
    "# # new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51831cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_loaded = None\n",
    "\n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "\n",
    "        if self.model_loaded is None:\n",
    "            print(\"LLM model loading from data section\")\n",
    "            self.model_loaded = pipeline(\"text-classification\", model=\"/data/finbert\")\n",
    "\n",
    "    def request_processing_fn(self, request) :\n",
    "        \"\"\"\n",
    "            Processes Request Object -> List[Input data, ..]. It could be:\n",
    "                   A List Mapping of All Value Can Be one of : \n",
    "                       - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "                       - List[numpy.array(), numpy.array(), ...]\n",
    "                       - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "                       - List[ SingleSample, SingleSample]\n",
    "\n",
    "            :return: (n_inputs, payload's)\n",
    "\n",
    "            Warnings:\n",
    "            1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "               Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "            \"\"\"\n",
    "        final_payload = []\n",
    "        raw_payload = request.json[\"payload\"]\n",
    "        return (1, raw_payload) \n",
    "\n",
    "    def pre_processing_fn(self,payload):\n",
    "        \n",
    "        #         # All preprocessing step must occur in this section\n",
    "        #         # Takes Single Sample -> Returns Single Sample\n",
    "\n",
    "        #         # Not Doing Any Preprocessing Hence Returned payload\n",
    "        #         print(\"payload is \", payload)\n",
    "        return payload\n",
    "\n",
    "    def prediction_fn(self,model: Any,input_query):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "\n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "\n",
    "        \"\"\"\n",
    "        model_loaded = self.model_loaded\n",
    "        mod = model_loaded\n",
    "        text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "\n",
    "\n",
    "        preds = pipe(input_query)\n",
    "        print(\"prediction is \\n: \",preds)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    #     class Meta:    \n",
    "         # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "    #         def __init__(self):\n",
    "    #             self.name = \"Pre Hooked Me !\"\n",
    "    #             self.pre_call_hooks.append(self.print_)\n",
    "    #         def print_(self):\n",
    "    #             print(self.name)\n",
    "    #             pre_call_hooks = []\n",
    "    #             post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc341beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model loading from data section\n"
     ]
    }
   ],
   "source": [
    "score_ = ScoreTemplateExample()\n",
    "import requests\n",
    "req = requests.Request()\n",
    "\n",
    "req.json = {\"payload\":\"['this stock is going to fall']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6ed4178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALIDATION</th>\n",
       "      <th>COMPONENT</th>\n",
       "      <th>PASSED</th>\n",
       "      <th>SKIPPED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Return Type Must Be Tuple (n_input, payloads)</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuple Must Be of length Two (n_input, payloads)</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* if n_input &gt; 1 payload type must be List (n_input, [np.ndarray, tf.Tensor, etc])</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           VALIDATION  \\\n",
       "0                                       Return Type Must Be Tuple (n_input, payloads)   \n",
       "1                                     Tuple Must Be of length Two (n_input, payloads)   \n",
       "2  * if n_input > 1 payload type must be List (n_input, [np.ndarray, tf.Tensor, etc])   \n",
       "\n",
       "               COMPONENT  PASSED  SKIPPED  \n",
       "0  request_processing_fn    True    False  \n",
       "1  request_processing_fn    True    False  \n",
       "2  request_processing_fn    True    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields Marked Asterisk (*) Can Be Validated On Proper Input \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pre_processed_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mscore_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m t4 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m t4\u001b[38;5;241m-\u001b[39mt3\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mosaic_utils/ai/score/base.py:147\u001b[0m, in \u001b[0;36mScoreBase.score\u001b[0;34m(self, model, input_request, dry_run)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_with_schema_payload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meach_pre_processed_schema,\n\u001b[1;32m    144\u001b[0m                                      _clean_payload):\n\u001b[1;32m    145\u001b[0m     _clean_payload_validated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m _score_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_clean_payload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m _response \u001b[38;5;241m=\u001b[39m ScoreResponse(score_request\u001b[38;5;241m=\u001b[39minput_request\u001b[38;5;241m.\u001b[39mjson,\n\u001b[1;32m    150\u001b[0m                           request_processed_payload\u001b[38;5;241m=\u001b[39meach_request,\n\u001b[1;32m    151\u001b[0m                           rp_payload_validated\u001b[38;5;241m=\u001b[39m_rp_payload_validated,\n\u001b[1;32m    152\u001b[0m                           clean_payload\u001b[38;5;241m=\u001b[39m_clean_payload,\n\u001b[1;32m    153\u001b[0m                           clean_payload_validated\u001b[38;5;241m=\u001b[39m_clean_payload_validated,\n\u001b[1;32m    154\u001b[0m                           score_response\u001b[38;5;241m=\u001b[39m_score_response)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_response_collection\u001b[38;5;241m.\u001b[39mappend(_response)\n",
      "Cell \u001b[0;32mIn[24], line 59\u001b[0m, in \u001b[0;36mScoreTemplateExample.prediction_fn\u001b[0;34m(self, model, input_query)\u001b[0m\n\u001b[1;32m     57\u001b[0m model_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_loaded\n\u001b[1;32m     58\u001b[0m mod \u001b[38;5;241m=\u001b[39m model_loaded\n\u001b[0;32m---> 59\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mpre_processed_input\u001b[49m \u001b[38;5;66;03m#this is tuple we can iterate if there is number of input\u001b[39;00m\n\u001b[1;32m     62\u001b[0m preds \u001b[38;5;241m=\u001b[39m pipe(input_query)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction is \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m,preds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_processed_input' is not defined"
     ]
    }
   ],
   "source": [
    "t3 = time.time()\n",
    "model_predictions = score_.score(None, req, dry_run=True)\n",
    "t4 = time.time()\n",
    "t4-t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "364fcb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<<ScoreResponse>>]\n"
     ]
    }
   ],
   "source": [
    "print(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3b7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:PASS:: Mandatory Validation :: dict_keys(['name', 'description', 'flavour', 'scoring_func'])\n",
      "INFO:root:PASS:: AlphaNumeric Validation :: dict_keys(['name'])\n",
      "INFO:root:PASS:: Validation/IfPresentTypeCheck :: dict_keys(['schema', 'metadata_info', 'tags'])\n",
      "INFO:root:PASS:: Validation/IfPresentSubfieldMustExist :: dict_keys(['kyd'])\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): mosaic-ai-backend:5000\n"
     ]
    }
   ],
   "source": [
    "register_model(model,\n",
    "               ScoreTemplateExample,\n",
    "               \"Starchat_Beta\",\n",
    "               \"Starchat_Beta_for_code_Generation\",\n",
    "               MLModelFlavours.pytorch,\n",
    "               init_script=\"pip install SentencePiece \\\\n pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9d796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
