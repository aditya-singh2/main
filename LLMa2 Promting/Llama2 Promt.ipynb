{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0305521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdcb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e661a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31391561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c11d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a421b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the sales in africa\"\n",
    "prompt = f\"\"\"[INST]<<SYS>>\n",
    "Assume you are a system that detects the month and year in the given secentance, user will give only provide the input in english.\n",
    "For example if user gives \"top  orders in march 2020\", you should only return march 2020.\n",
    "\n",
    "<</SYS>>\n",
    "{question}\n",
    "[/INST]\n",
    "[END]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71190d63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8015e366e1334e519ff82c2b76b5924d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[INST]<<SYS>>\\nAssume you are a system that detects the month and year in the given secentance, user will give only provide the input in english.\\nFor example if user gives \\\"top  orders in march 2020\\\", you should only return march 2020.\\n\\n<</SYS>>\\nwhat are the sales in africa\\n[/INST]\\n[END]\"\n",
      "\n",
      "The input provided is \"what are the sales in africa\". Based on the given input, I can detect the month and year as \"March 2020\". Therefore, the output will be \"March 2020\".\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "BASE_MODEL = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = \"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt = json.dumps(prompt)\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#model.eval()\n",
    "response = tokenizer.decode(model.generate(**model_input, max_length=4096)[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2c27a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"[INST]<<SYS>>\\\\nAssume you are a system that detects the month and year in the given secentance, user will give only provide the input in english.\\\\nFor example if user gives \\\\\"top  orders in march 2020\\\\\", you should only return march 2020.\\\\n\\\\n<</SYS>>\\\\nwhat are the sales in africa\\\\n[/INST]\\\\n[END]\"\\n\\nThe input provided is \"what are the sales in africa\". Based on the given input, I can detect the month and year as \"March 2020\". Therefore, the output will be \"March 2020\".'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04447e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
