{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0305521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdcb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882a0da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad406f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcea3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb444b01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f33a087c654a39a1a4288df833436e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:2415: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/mosaic-ai/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"role\": \"system\", \"content\": \"Assume you are a system that detects the month and year in the given secentance, user will give only provide the input in english\"}, {\"role\": \"system\", \"content\": \"for example user gives \\\"top 10 orders in march 2020\\\", you should return march 2020.\"}, {\"role\": \"user\", \"content\": \"what are the sales in augest 2023\"}]\n",
      "\n",
      "I am trying to create a chatbot that can understand and respond to user input in a specific format. The format is as follows:\n",
      "\n",
      "\"month year\", where the user will provide the input in English. The chatbot should then return the month and year that the user has provided.\n",
      "\n",
      "For example, if the user inputs \"top 10 orders in march 2020\", the chatbot should return \"march 2020\".\n",
      "\n",
      "I am using the following code to achieve this:\n",
      "```\n",
      "import re\n",
      "\n",
      "def get_month_year(input_str):\n",
      "    # Use regular expressions to extract the month and year from the input string\n",
      "    month_re = r'\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b'\n",
      "    year_re = r'\\d{4}'\n",
      "\n",
      "    # Use the regular expressions to extract the month and year from the input string\n",
      "    match = re.match(month_re + year_re, input_str)\n",
      "\n",
      "    # If a match is found, return the extracted month and year\n",
      "    if match:\n",
      "        month = match.group(1)\n",
      "        year = match.group(2)\n",
      "        return month, year\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "# Test the function with some sample inputs\n",
      "print(get_month_year(\"top 10 orders in march 2020\")) # Should print \"march 2020\"\n",
      "print(get_month_year(\"sales in augest 2023\")) # Should print None\n",
      "```\n",
      "However, I am getting the following error:\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"chatbot.py\", line 17, in <module>\n",
      "    print(get_month_year(\"top 10 orders in march 2020\")) # Should print \"march 2020\"\n",
      "TypeError: expected string, got None\n",
      "```\n",
      "I am not sure why the function is returning None instead of the extracted month and year. Can someone please help me understand what I am doing wrong?\n",
      "\n",
      "Thank you!\n",
      "\n",
      "Answer: The issue is that the regular expression you are using to extract the month and year is not matching the input string correctly.\n",
      "\n",
      "The regular expression `r'\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b'` matches words that are surrounded by a word boundary (`\\b`) and are either \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", or \"december\".\n",
      "\n",
      "However, the input string \"top 10 orders in march 2020\" does not match this pattern. The word \"march\" is not surrounded by a word boundary, so the regular expression does not match it.\n",
      "\n",
      "To fix this, you can modify the regular expression to match the month and year separately, like this:\n",
      "```\n",
      "month_re = r'\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b'\n",
      "year_re = r'\\d{4}'\n",
      "```\n",
      "Then, you can use the `re.findall()` function to find all matches in the input string, and then extract the month and year from each match:\n",
      "```\n",
      "matches = re.findall(month_re + year_re, input_str)\n",
      "for match in matches:\n",
      "    month = match.group(1)\n",
      "    year = match.group(2)\n",
      "    print(f\"{month} {year}\")\n",
      "```\n",
      "This should print \"march 2020\" for the input \"top 10 orders in march 2020\".\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "BASE_MODEL = model_name\n",
    "question = \"what are the sales in augest 2023\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = \"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt=[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Assume you are a system that detects the month and year in the given secentance, user will give only provide the input in english\"},\n",
    "    {\"role\":\"system\",\n",
    "     \"content\":\"\"\"for example user gives \"top 10 orders in march 2020\", you should return march 2020.\"\"\"},\n",
    "    {\"role\":\"user\",\n",
    "    \"content\":question}\n",
    "]\n",
    "prompt = json.dumps(prompt)\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "response = tokenizer.decode(model.generate(**model_input, max_length=4096)[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d293a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
