{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0305521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "import json\n",
    "from query_to_mql_tt_integrated import system_msg,context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdcb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3957d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6d6830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1970747df7ea4c7b9982fc9f605f9d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config,\n",
    "    device_map=device_map)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a7a18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing prompt\n",
    "\n",
    "# question = \"what are the sales in africa\"\n",
    "# prompt = f\"\"\"[INST]<<SYS>>\n",
    "# Assume you are a system that detects the month and year in the given secentance, user will give only provide the input in english.\n",
    "# For example if user gives \"top  orders in march 2020\", you should only return march 2020.\n",
    "\n",
    "# <</SYS>>\n",
    "# {question}\n",
    "# [/INST]\n",
    "# [END]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121ba1af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gereneate_output_llama2(prompt):\n",
    "    #prompt = json.dumps(prompt)\n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    #model.eval()\n",
    "    response = tokenizer.decode(model.generate(**model_input, max_length=4096)[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3891448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_question(question):\n",
    "    return \"Based on the following context:\\n\" + str(context) + \"\\n return a response for the question:\\n\" + question + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642d98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_prompt(question):\n",
    "    return f\"\"\"[INST]<<SYS>>\n",
    "            {system_msg}\n",
    "            <</SYS>>\n",
    "            {get_user_question(question)}\n",
    "            [/INST]\n",
    "            [END]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aadd3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any question:when was quantity greater than 10k for phone\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter any question:\" )\n",
    "output=gereneate_output_llama2(compose_prompt(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fb0977c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nThe response for the question \"When was quantity greater than 10k for phone?\" would be:\\n\\n{\\n\"MEASURE\": [\\n    {\\n        \"ENTITY\": \"Discount\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    },\\n    {\\n        \"ENTITY\": \"Purchase Vol\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    },\\n    {\\n        \"ENTITY\": \"Quantity\",\\n        \"RANK\": [{\"RANK ADJECTIVE\": \"greater\", \"RANK VALUE\": \"10k\"}],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    },\\n    {\\n        \"ENTITY\": \"Sales\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    }\\n],\\n\"DIMENSION\": [\\n    {\\n        \"ENTITY\": \"Sub-Category\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    },\\n    {\\n        \"ENTITY\": \"Segment\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    },\\n    {\\n        \"ENTITY\": \"Parts\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    },\\n    {\\n        \"ENTITY\": \"Country\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    }\\n],\\n\"FILTER\": [\\n    {\\n        \"ENTITY\": \"Consumer\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    },\\n    {\\n        \"ENTITY\": \"Phone\",\\n        \"RANK\": [{\"RANK ADJECTIVE\": \"greater\", \"RANK VALUE\": \"10k\"}],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    }\\n],\\n\"DERIVED MEASURE\": [\\n    {\\n        \"ENTITY\": \"Ratio\",\\n        \"RANK\": [{\"RANK ADJECTIVE\": \"greater\", \"RANK VALUE\": \"10k\"}],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    }\\n],\\n\"DATE VARIABLE\": [\\n    {\\n        \"ENTITY\": \"Order Date\",\\n        \"RANK\": [],\\n        \"ADJECTIVE\": [],\\n        \"TONE\": \"\"\\n    }\\n]\\n}\\n\\nReasoning:\\n\\nBased on the context provided, the question is asking for the date when the quantity of phones was greater than 10k. To answer this question, we need to identify the n-grams that match between the question and the context.\\n\\nThe n-grams that match between the question and the context are:\\n\\n* \"phone\" (matched with the \"ENTITY\" of \"Phone\" in the context)\\n* \"greater\" (matched with the \"RANK ADJECTIVE\" of \"greater\" and \"RANK VALUE\" of \"10k\" in the context)\\n\\nSince there are only two n-grams that match, we return only the first match. The response includes the identified n-grams and their corresponding ranks, as well as the other conditions that match between the question and the context.\\n\\nFor the \"DATE VARIABLE\" section, we provide the \"Order Date\" as the date range, as it is the only date variable that matches with the question.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.split(\"[END]\")[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
