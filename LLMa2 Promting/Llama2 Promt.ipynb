{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0305521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdcb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321cb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b505f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d841cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560f67a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4059a40d6da94036bb8c96186b6c0007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:2415: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/mosaic-ai/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"role\": \"system\", \"content\": \"Assume you are a system that detects the month and year in the given secentance, user will give only provide the input in english\"}, {\"role\": \"system\", \"content\": \"for example user gives \\\"top  orders in march 2020\\\", you should return march 2020.\"}, {\"role\": \"user\", \"content\": \"what are the sales in august 2023\"}]\n",
      "\n",
      "I am trying to create a chatbot that can understand and respond to user input in a specific format. The format is as follows:\n",
      "\n",
      "\"month year\", where the user will provide the input in English. The chatbot should then return the month and year that the user has provided.\n",
      "\n",
      "For example, if the user inputs \"top orders in march 2020\", the chatbot should return \"march 2020\".\n",
      "\n",
      "I am using the following code to achieve this:\n",
      "```\n",
      "import re\n",
      "\n",
      "def get_month_year(input_str):\n",
      "    # Use regular expressions to extract the month and year from the input string\n",
      "    month_year_pattern = r\"(\\w+ \\w+)\"\n",
      "    match = re.match(month_year_pattern, input_str)\n",
      "    if match:\n",
      "        # Extract the month and year from the match object\n",
      "        month = match.group(1)\n",
      "        year = match.group(2)\n",
      "        return month, year\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "# Test the function with some sample inputs\n",
      "print(get_month_year(\"top orders in march 2020\"))  # Output: march 2020\n",
      "print(get_month_year(\"sales in august 2023\"))  # Output: august 2023\n",
      "```\n",
      "However, I am getting the following error:\n",
      "```\n",
      "Traceback (most recent call last):\n",
      "  File \"chatbot.py\", line 17, in <module>\n",
      "    print(get_month_year(\"sales in august 2023\"))\n",
      "  File py:13, in get_month_year\n",
      "    match = re.match(month_year_pattern, input_str)\n",
      "AttributeError: 'NoneType' object has no attribute 'group'\n",
      "```\n",
      "I am not sure why the regular expression is not matching the input string. Can someone please help me understand what I am doing wrong?\n",
      "\n",
      "Answer: The issue is that the regular expression you are using `r\"(\\w+ \\w+)\"` matches any sequence of one or more word characters (letters, digits, or underscores) followed by any sequence of one or more word characters. However, in your input string \"sales in august 2023\", the words \"sales\" and \"august\" are not separated by any whitespace characters, so the regular expression does not match them.\n",
      "\n",
      "To fix this, you can modify the regular expression to match any sequence of one or more word characters that are separated by any whitespace character, like this:\n",
      "```\n",
      "month_year_pattern = r\"(\\w+ \\s+ \\w+)\"\n",
      "```\n",
      "This will match any sequence of one or more word characters that are separated by any amount of whitespace characters (spaces, tabs, or line breaks).\n",
      "\n",
      "Alternatively, you can use a more flexible regular expression that matches any sequence of word characters that are separated by any amount of whitespace characters, like this:\n",
      "```\n",
      "month_year_pattern = r\"(\\w+ \\S+ \\w+)\"\n",
      "```\n",
      "This will match any sequence of one or more word characters that are separated by any amount of non-whitespace characters (letters, digits, or underscores), which should cover most input strings that contain both words and whitespace characters.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "BASE_MODEL = model_name\n",
    "question = \"what are the sales in august 2023\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = \"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt=[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Assume you are a system that detects the month and year in the given secentance, user will give only provide the input in english\"},\n",
    "    {\"role\":\"system\",\n",
    "     \"content\":\"\"\"for example if user gives \"top  orders in march 2020\", you should return march 2020.\"\"\"},\n",
    "    {\"role\":\"user\",\n",
    "    \"content\":question}\n",
    "]\n",
    "prompt = json.dumps(prompt)\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "response = tokenizer.decode(model.generate(**model_input, max_length=4096)[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d355a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
