{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0305521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdcb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02345d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ad8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832ca739",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebda154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the sales in august 2023\"\n",
    "prompt = f\"\"\"[INST]<<SYS>>\n",
    "Assume you are a system that detects the month and year in the given secentance, user will give only provide the input in english.\n",
    "For example if user gives \"top  orders in march 2020\", you should only return march 2020. In the output only show \"march 2020\"\n",
    "\n",
    "<</SYS>>\n",
    "{question}\n",
    "[/INST]\n",
    "[END]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cbc4338",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5785e099423041908a9cf2f71e1c3889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[INST]<<SYS>>\\nAssume you are a system that detects the month and year in the given secentance, user will give only provide the input in english.\\nFor example if user gives \\\"top  orders in march 2020\\\", you should only return march 2020. In the output only show \\\"march 2020\\\"\\n\\n<</SYS>>\\nwhat are the sales in august 2023\\n[/INST]\\n[END]\"\n",
      "\n",
      "The output for the given input would be:\n",
      "\n",
      "\"march 2020\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "BASE_MODEL = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = \"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt = json.dumps(prompt)\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "response = tokenizer.decode(model.generate(**model_input, max_length=4096)[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc4d9a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"[INST]<<SYS>>\\\\nAssume you are a system that detects the month and year in the given secentance, user will give only provide the input in english.\\\\nFor example if user gives \\\\\"top  orders in march 2020\\\\\", you should only return march 2020. in the output only show \\\\\"march 2020\\\\\" \\\\nno need to show other nlg. If no month or year detected just show \\\\\"no month or year is detected\\\\\" \\\\n<</SYS>>\\\\n\\\\nwhat are the sales in august 2023\\\\n[/INST]\\\\n[END]\"no month or year is detected\"] '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
