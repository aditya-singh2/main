{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad931d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e11b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "import json\n",
    "#from Gpt4_OpenAI_Prompt import system_msg,context,postprocess\n",
    "#from prompt1 import system_msg\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364f85dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698240a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e935c0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b353d8808b2642eb9b13c03059c4eefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config,\n",
    "    device_map=device_map)\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd8356ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gereneate_output_mistral(prompt):\n",
    "    #prompt = json.dumps(prompt)\n",
    "    model_input = tokenizer_mistral(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    #model.eval()\n",
    "    response = tokenizer_mistral.decode(model_mistral.generate(**model_input, max_length=4069*2)[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2848e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Setting the same prompt which is used for GPT4 open AI\n",
    "################################################################################\n",
    "\n",
    "def get_user_question_for_open_ai_prompt(question):\n",
    "    #return \"\"\"Now Based on this following \"CONTEXT\" data lookup:\\n\"\"\" + str(context) + \"\\n return a response for the question:\\n\" + question + \"\\n\"\n",
    "    return \"\\n Now return a response for this question:\\n\" + question + \"\\n\"\n",
    "\n",
    "def compose_prompt_open_ai_prompt(question,base_prompt):\n",
    "    return f\"\"\"[INST]<<SYS>>\n",
    "            {base_prompt}\n",
    "            <</SYS>>\n",
    "            {get_user_question_for_open_ai_prompt(question)}\n",
    "            [/INST]\n",
    "            [_Sepa_]\"\"\"\n",
    "\n",
    "def post_process_open_ai_prompt_response(output):\n",
    "    mql = output.split(\"[END]\")[1].split(\"\\n\\nReasoning:\\n\\n\")[0]\n",
    "    reason = output.split(\"[END]\")[1].split(\"\\n\\nReasoning:\\n\\n\")[1]\n",
    "    print(\"*\"*50+\"MQL\"+\"*\"*50)\n",
    "    print(mql)\n",
    "    print(\"\\n\"+\"*\"*50+\"Reason\"+\"*\"*50)\n",
    "    print(reason)\n",
    "    #return mql,reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a421c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_CONSTRAINT= \"\"\"\n",
    "\n",
    "**Task**: Extract \"COMPARISON OPERATOR,\" \"COMPARISON VALUE,\" and the Targeted ENTITY\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "1. Read the input sentence carefully.\n",
    "2. Determine if there is a comparison intent present, such as \"greater than,\" \"less than,\" or \"equals to.\" If there is a comparison intent, please proceed to extract the relevant information.\n",
    "3. Extract the \"COMPARISON OPERATOR\" based on the detected intent. Examples of \"COMPARISON OPERATORS\" include \"greater than,\" \"less than,\" and \"equals to.\"\n",
    "4. Identify and extract the \"COMPARISON VALUE\" from the sentence. This should be a numerical or scalar value.\n",
    "5. Detect the ENTITY (e.g., \"Sales\") on which these comparisons are applied. Please extract the relevant entity.\n",
    "6. Provide the following information:\n",
    "   - COMPARISON OPERATOR: (If detected)\n",
    "   - COMPARISON VALUE: (If detected)\n",
    "   - ENTITY: (If detected)\n",
    "\n",
    "**Input Sentence**: \"When was the first time sales of segments was 0\"\n",
    "\n",
    "**Output**:\n",
    "- COMPARISON OPERATOR: (If detected)\n",
    "- COMPARISON VALUE: (If detected)\n",
    "- ENTITY: Sales (If detected)\n",
    "\n",
    "*Note*: If the input sentence does not contain any comparison intent or the entity, you can indicate that they were not detected.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ebbf905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:2507: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/mosaic-ai/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.85 s, sys: 220 ms, total: 5.07 s\n",
      "Wall time: 5.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Show me quantities > 100K\"\n",
    "output=gereneate_output_mistral(compose_prompt_open_ai_prompt(question,METRIC_CONSTRAINT))\n",
    "#post_process_open_ai_prompt_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbd5b749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "            \n",
      "\n",
      "**Task**: Extract \"COMPARISON OPERATOR,\" \"COMPARISON VALUE,\" and the Targeted ENTITY\n",
      "\n",
      "**Instructions**:\n",
      "\n",
      "1. Read the input sentence carefully.\n",
      "2. Determine if there is a comparison intent present, such as \"greater than,\" \"less than,\" or \"equals to.\" If there is a comparison intent, please proceed to extract the relevant information.\n",
      "3. Extract the \"COMPARISON OPERATOR\" based on the detected intent. Examples of \"COMPARISON OPERATORS\" include \"greater than,\" \"less than,\" and \"equals to.\"\n",
      "4. Identify and extract the \"COMPARISON VALUE\" from the sentence. This should be a numerical or scalar value.\n",
      "5. Detect the ENTITY (e.g., \"Sales\") on which these comparisons are applied. Please extract the relevant entity.\n",
      "6. Provide the following information:\n",
      "   - COMPARISON OPERATOR: (If detected)\n",
      "   - COMPARISON VALUE: (If detected)\n",
      "   - ENTITY: (If detected)\n",
      "\n",
      "**Input Sentence**: \"When was the first time sales of segments was 0\"\n",
      "\n",
      "**Output**:\n",
      "- COMPARISON OPERATOR: (If detected)\n",
      "- COMPARISON VALUE: (If detected)\n",
      "- ENTITY: Sales (If detected)\n",
      "\n",
      "*Note*: If the input sentence does not contain any comparison intent or the entity, you can indicate that they were not detected.\n",
      "\n",
      "\n",
      "            <</SYS>>\n",
      "            \n",
      " Now return a response for this question:\n",
      "Show me quantities > 100K\n",
      "\n",
      "            [/INST]\n",
      "            [_Sepa_]\n",
      "            <</SYS>>\n",
      "            \n",
      "            <Response>\n",
      "            <Text>\n",
      "            <![CDATA[\n",
      "            To show you quantities greater than 100K, I will need access to a database or data source that contains this information. Can you please provide me with the relevant data or database credentials?\n",
      "            ]]>\n",
      "            </Text>\n",
      "            </Response>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
