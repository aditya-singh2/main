{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fbc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bb11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('template_and_user_query.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a6d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>user query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>What were the 2020 sales figures?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>How much were the sales in 2020?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>What was the sales volume in 2020?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>What did the 2020 sales amount to?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>What were the total sales for 2020?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    question                           user query\n",
       "0  What is the sales in 2020    What were the 2020 sales figures?\n",
       "1  What is the sales in 2020     How much were the sales in 2020?\n",
       "2  What is the sales in 2020   What was the sales volume in 2020?\n",
       "3  What is the sales in 2020   What did the 2020 sales amount to?\n",
       "4  What is the sales in 2020  What were the total sales for 2020?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9060e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1de447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37f45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "#dataset_name = \"\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-fine-tuned-peft-on-template_and_user_query-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2ceeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83e1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b95b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"/data/sept-23\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 15\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 1000\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 30\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd83a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e97220",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1 = \"\"\"### _LUMIN_ Question:\n",
    "{question}\n",
    "### Context: {context}\n",
    "### _LUMIN_ Answer: {answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8b6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt_template_v2 = \"\"\"[INST]<<SYS>>\n",
    "You are an advanced template converter that converts user question to a specific template which answers the user question.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{user_query}\n",
    "[/INST]\n",
    "[LUMINTEMPLATE]\n",
    "{template_query}\n",
    "[/LUMINTEMPLATE]</s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eec8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"### Question:\n",
    "{user_query}\n",
    "\n",
    "### Context:\n",
    "Converting above _LUMIN_ Question to LUMIN specific template _LUMIN_ Answer.\n",
    "\n",
    "### Answer:\n",
    "{template_query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f215307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Converting above _LUMIN_ Question to LUMIN specific template _LUMIN_ Answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d534e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'user query'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a95227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fine_tuning_dataset(row):\n",
    "    user_query = row['user query']\n",
    "    template_query = row['question']\n",
    "    formated = promt_template_v2.format(user_query=user_query,\n",
    "                                      template_query=template_query)\n",
    "    # data_point = str({'question': formated, \"answer\":template_query})\n",
    "    return formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6859de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fine_tuning_dataset']=df.apply(create_fine_tuning_dataset, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af15320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST]<<SYS>>\\nYou are an advanced template converter that converts user question to a specific template which answers the user question.\\n\\n<</SYS>>\\n\\n2020 and 2021: What's the difference in sales and discount percentages?\\n[/INST]\\n[LUMINTEMPLATE]\\nWhat is the sales and discount percentage in 2020 vs 2021\\n[/LUMINTEMPLATE]</s>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fine_tuning_dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1650ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['question','user query'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f60dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4c4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df[:2100])\n",
    "val_dataset = Dataset.from_pandas(df[2100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "555e910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fine_tuning_dataset'],\n",
       "    num_rows: 280\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68cc3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed25c775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nf4'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_4bit_quant_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6664ecee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e2c3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba0a15e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039171c54c924566877af935ecef71d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef60f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                          # add_eos_token=True,\n",
    "                                          use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6716c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer.encode(df['fine_tuning_dataset'][i])) for i in range(2380)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fc5bf25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(df['fine_tuning_dataset'][30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92a7efe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25af47a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75ffc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e30b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    eval_steps=30,\n",
    "    per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=1000,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de52effd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fine_tuning_dataset'],\n",
       "    num_rows: 2100\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8bf5981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54cde460fce40f9944e9ab99ef653c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b10bf74ebc4cd09b48e0aff5a0d0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"fine_tuning_dataset\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d942d91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fine_tuning_dataset': \"[INST]<<SYS>>\\nYou are an advanced template converter that converts user question to a specific template which answers the user question.\\n\\n<</SYS>>\\n\\n2020 and 2021: What's the difference in sales and discount percentages?\\n[/INST]\\n[LUMINTEMPLATE]\\nWhat is the sales and discount percentage in 2020 vs 2021\\n[/LUMINTEMPLATE]</s>\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5769042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 451/1000 2:14:27 < 2:44:24, 0.06 it/s, Epoch 1.71/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.711800</td>\n",
       "      <td>0.736779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.359340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.333100</td>\n",
       "      <td>0.295548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>0.280266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.280061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.251686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.249138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.227733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.224117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.221874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.198600</td>\n",
       "      <td>0.220921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.203100</td>\n",
       "      <td>0.215807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.210381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.212715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.204487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-fine-tuned-peft-v3-new-template\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5cbfc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84916548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99617b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_merge = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    'llama-2-7b-fine-tuned-peft-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d4d7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"### Question:\n",
    "{user_query}\n",
    "\n",
    "### Context:\n",
    "Converting above _LUMIN_ Question to LUMIN specific template _LUMIN_ Answer.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "670ccb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_template = \"\"\"### Instruction:\n",
    "# Converting below input to LUMIN understandable format.\n",
    "# ### Input:\n",
    "# {user_query}\n",
    "# ### Response:\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb61f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load LLaMA tokenizer\n",
    "# query_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,\n",
    "#                                           add_eos_token=False,\n",
    "#                                           use_fast=True)\n",
    "# #tokenizer.pad_token = tokenizer.eos_token\n",
    "# query_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "196a3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_template_query(user_query):\n",
    "    question = query_template.format(user_query=user_query)\n",
    "    _input = str({'question': question})\n",
    "#     print(_input)\n",
    "    _inputs = tokenizer.encode(_input, return_tensors=\"pt\")\n",
    "#     print(_inputs)\n",
    "#     print(type(_inputs))\n",
    "    outputs = model_to_merge.generate(input_ids=_inputs.to('cuda'), max_length= 200)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    # print(output)\n",
    "    # a = output.split(\"_LUMIN_ Answer\",1)[1]\n",
    "    #print(a)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7323583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(a):\n",
    "    b=a.split(\"### Answer\",1)[1]\n",
    "    return b.split(\"\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f69bfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '### Question:\\nWhat were the 2020 sales figures?\\n\\n### Context:\\nConverting above _LUMIN_ Question to LUMIN specific template _LUMIN_ Answer.\\n\\n'}\n",
      "tensor([[    1, 11117, 12470,  2396,   525,  2277, 29937,   894,  3583, 29876,\n",
      "          5618,   892,   278, 29871, 29906, 29900, 29906, 29900, 16538, 13994,\n",
      "         29973, 29905, 29876, 29905, 29876,  2277, 29937, 15228,  3583, 29876,\n",
      "          1168,   369,  1259,  2038,   903, 29931,  5005,  1177, 29918,   894,\n",
      "           304,   365,  5005,  1177,  2702,  4472,   903, 29931,  5005,  1177,\n",
      "         29918,   673,  7790, 29876, 29905, 29876, 10827]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "inp = 'What were the 2020 sales figures?'\n",
    "a=process_output(predict_template_query(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65288dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.split(\"### Answer\",1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73d16562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the sales in 2020'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.split(\"\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84909106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('template_and_user_query.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb192915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5a6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [15:43, 102.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got exception while processing What were the sales numbers for 2020?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [15:59, 75.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got exception while processing How much revenue was generated from sales in 2020?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [16:27, 60.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got exception while processing What was the total sales income in 2020?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [16:31, 43.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got exception while processing What are the 2020 monthly sales figures?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [18:06, 59.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got exception while processing How much were sales in each month of 2020?\n"
     ]
    }
   ],
   "source": [
    "output_list = []\n",
    "for i, row in tqdm(df2.iterrows()):\n",
    "    inp = row['user query']\n",
    "    try:\n",
    "        output = process_output(predict_template_query(inp))\n",
    "    except:\n",
    "        output = None\n",
    "        print(f\"Got exception while processing : {inp}\")\n",
    "    output_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476faf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972fa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c353855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1a7769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = \"why production dropped for bikes in 1st quarter 2022\"\n",
    "# predict_template_query(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac29f8f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inp = 'why sales of texas increase in 1st quarter 2023'\n",
    "# a = predict_template_query(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e75fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6bd05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e68c1d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inp = 'drivers of profit'\n",
    "# predict_template_query(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1023e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = \"why sales dropped for bikes in 1st quarter 2022\"\n",
    "# predict_template_query(inp).split('\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9f621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f2759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
