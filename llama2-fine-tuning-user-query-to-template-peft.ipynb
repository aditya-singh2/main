{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fbc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02bb11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('template_and_user_query.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a6d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>user query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>What were the 2020 sales figures?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>How much were the sales in 2020?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>What was the sales volume in 2020?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>What did the 2020 sales amount to?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the sales in 2020</td>\n",
       "      <td>What were the total sales for 2020?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    question                           user query\n",
       "0  What is the sales in 2020    What were the 2020 sales figures?\n",
       "1  What is the sales in 2020     How much were the sales in 2020?\n",
       "2  What is the sales in 2020   What was the sales volume in 2020?\n",
       "3  What is the sales in 2020   What did the 2020 sales amount to?\n",
       "4  What is the sales in 2020  What were the total sales for 2020?"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9060e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1de447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37f45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "#dataset_name = \"\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-fine-tuned-peft-on-template_and_user_query-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2ceeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83e1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b95b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"/data/sept-23\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 15\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 1000\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 30\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd83a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e97220",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1 = \"\"\"### _LUMIN_ Question:\n",
    "{question}\n",
    "### Context: {context}\n",
    "### _LUMIN_ Answer: {answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b17057",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt_template_v2 = \"\"\"[INST]<<SYS>>\n",
    "You are an advanced template converter that converts user question to a specific template which answers the user question.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{user_query}\n",
    "[/INST]\n",
    "[LUMINTEMPLATE]\n",
    "{template_query}\n",
    "[/LUMINTEMPLATE]</s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eec8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"### Question:\n",
    "{user_query}\n",
    "\n",
    "### Context:\n",
    "Converting above _LUMIN_ Question to LUMIN specific template _LUMIN_ Answer.\n",
    "\n",
    "### Answer:\n",
    "{template_query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f215307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Converting above _LUMIN_ Question to LUMIN specific template _LUMIN_ Answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d534e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'user query'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a95227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fine_tuning_dataset(row):\n",
    "    user_query = row['user query']\n",
    "    template_query = row['question']\n",
    "    formated = promt_template_v2.format(user_query=user_query,\n",
    "                                      template_query=template_query)\n",
    "    # data_point = str({'question': formated, \"answer\":template_query})\n",
    "    return formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6859de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fine_tuning_dataset']=df.apply(create_fine_tuning_dataset, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af15320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST]<<SYS>>\\nYou are an advanced template converter that converts user question to a specific template which answers the user question.\\n\\n<</SYS>>\\n\\nWhen did sales reach their minimum in 2020?\\n[/INST]\\n[LUMINTEMPLATE]\\nWhen was sales lowest in 2020\\n[/LUMINTEMPLATE]</s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fine_tuning_dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1650ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['question','user query'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f60dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2380, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4c4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df[:2100])\n",
    "val_dataset = Dataset.from_pandas(df[2100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "555e910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fine_tuning_dataset'],\n",
       "    num_rows: 280\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68cc3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed25c775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nf4'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_4bit_quant_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6664ecee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e2c3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba0a15e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c58e3ccd1e34a60b76dc5967aab97ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef60f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,\n",
    "                                          # add_eos_token=True,\n",
    "                                          use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6716c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(tokenizer.encode(df['fine_tuning_dataset'][i])) for i in range(2380)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fc5bf25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(df['fine_tuning_dataset'][30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92a7efe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9379705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75ffc6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e30b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    eval_steps=30,\n",
    "    per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=1000,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de52effd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fine_tuning_dataset'],\n",
       "    num_rows: 2100\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8bf5981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e785b2ed16ec4807af6b54ccfa4b832d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333b152dc4404b4490e3bf6daf308413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"fine_tuning_dataset\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d942d91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fine_tuning_dataset': '[INST]<<SYS>>\\nYou are an advanced template converter that converts user question to a specific template which answers the user question.\\n\\n<</SYS>>\\n\\nWhen did sales reach their minimum in 2020?\\n[/INST]\\n[LUMINTEMPLATE]\\nWhen was sales lowest in 2020\\n[/LUMINTEMPLATE]</s>'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5769042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='991' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 991/1000 5:00:30 < 02:44, 0.05 it/s, Epoch 3.77/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.711800</td>\n",
       "      <td>0.736779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.359340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.333100</td>\n",
       "      <td>0.295548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>0.280266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.280061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.251686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.249138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.227733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.186400</td>\n",
       "      <td>0.224117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.221874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.198600</td>\n",
       "      <td>0.220921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.203100</td>\n",
       "      <td>0.215807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.210381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.212715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.204487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.198800</td>\n",
       "      <td>0.208522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.220300</td>\n",
       "      <td>0.208837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.203076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.201418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.164000</td>\n",
       "      <td>0.199660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.196334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.196823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.199650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.193322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.191863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.198800</td>\n",
       "      <td>0.191948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.193839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.188900</td>\n",
       "      <td>0.198292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.190829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.187063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.190828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>0.188071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.199600</td>\n",
       "      <td>0.192003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-fine-tuned-peft-v3-new-template\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5cbfc1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84916548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99617b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_merge = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    'llama-2-7b-fine-tuned-peft-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da27f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_merge = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    '/data/sept-23/checkpoint-900')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d4d7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"### Question:\n",
    "{user_query}\n",
    "\n",
    "### Context:\n",
    "Converting above _LUMIN_ Question to LUMIN specific template _LUMIN_ Answer.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7be81f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template_v2 = \"\"\"[INST]<<SYS>>\n",
    "You are an advanced template converter that converts user question to a specific template which answers the user question.\n",
    "\n",
    "<</SYS>>\n",
    "\n",
    "{user_query}\n",
    "[/INST]\n",
    "[LUMINTEMPLATE]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6df3bf36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_merge.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d990651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_template_query_v3(user_query):\n",
    "    inp = query_template_v2.format(user_query=user_query)\n",
    "    _inputs = tokenizer.encode(inp, return_tensors=\"pt\")\n",
    "    outputs = model_to_merge.generate(input_ids=_inputs.to('cuda'), max_length= 200)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    output_new = output.split('[LUMINTEMPLATE]\\n')[1]\n",
    "    return output_new.split('\\n[/LUMINTEMPLATE]')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6518c73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 40.6 s, total: 2min 4s\n",
      "Wall time: 2min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the sales in 2020'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predict_template_query_v3('What were the total sales for 2020?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39eb1d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 50 s, total: 2min 37s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "inp = 'what are you doing'\n",
    "inp = tokenizer.encode(inp, return_tensors='pt')\n",
    "output = model.generate(input_ids=inp.to('cuda'), max_length= 200)\n",
    "output = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb192915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8b34591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('template_and_user_query.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19815c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2_new = df2[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5a6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [2:41:04, 130.68s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_list_100_200 = []\n",
    "for i, row in tqdm(df2_new.iterrows()):\n",
    "    inp = row['user query']\n",
    "    try:\n",
    "        output = predict_template_query_v3(inp)\n",
    "    except:\n",
    "        output = None\n",
    "        print(f\"Got exception while processing : {inp}\")\n",
    "    output_list_100_200.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4fcdf37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output_list, columns=['predicted_template'])\n",
    "df.to_csv('predict_df1_0_100.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fbb3d6cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales across months in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales by sub category across months in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales of phone across months in 2020',\n",
       " 'What is the sales by sub category in 2020 vs 2021',\n",
       " 'What is the sales by sub category in 2020 vs 2021',\n",
       " 'What is the sales by sub category in 2020 vs 2021',\n",
       " 'What is the sales by sub category in 2020 vs 2021',\n",
       " 'What is the sales by sub category in 2020 vs 2021',\n",
       " 'What is the sales by sub category in 2020 vs 2021',\n",
       " 'Diagnose sales by sub category in 2020 vs 2021',\n",
       " 'What is the sales by sub category in 2020 vs 2021',\n",
       " 'What is the trend of sales by sub category in 2020 vs 2021',\n",
       " 'What is the sales by sub category in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales in 2020 vs 2021',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the contribution of sales and discounts in 2020',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the sales and discount percentage in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020',\n",
       " 'What is the sales and discount percentage by sub category in 2020']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ccb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550613d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61f802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "196a3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_template_query(user_query):\n",
    "    question = query_template.format(user_query=user_query)\n",
    "    _input = str({'question': question})\n",
    "#     print(_input)\n",
    "    _inputs = tokenizer.encode(_input, return_tensors=\"pt\")\n",
    "#     print(_inputs)\n",
    "#     print(type(_inputs))\n",
    "    outputs = model_to_merge.generate(input_ids=_inputs.to('cuda'), max_length= 200)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    # print(output)\n",
    "    # a = output.split(\"_LUMIN_ Answer\",1)[1]\n",
    "    #print(a)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_merge.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7323583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(a):\n",
    "    b=a.split(\"### Answer\",1)[1]\n",
    "    return b.split(\"\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476faf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972fa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c353855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1a7769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = \"why production dropped for bikes in 1st quarter 2022\"\n",
    "# predict_template_query(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac29f8f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inp = 'why sales of texas increase in 1st quarter 2023'\n",
    "# a = predict_template_query(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e75fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6bd05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e68c1d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inp = 'drivers of profit'\n",
    "# predict_template_query(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1023e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = \"why sales dropped for bikes in 1st quarter 2022\"\n",
    "# predict_template_query(inp).split('\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9f621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f2759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
