{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22e05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/aa/a89864288afe45abe1ab79f002140a20348140e86836d96096d8f8a3bac0/transformers-4.29.2-py3-none-any.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 9.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/02/a2cff6306177ae6bc73bc0665065de51dfb3b9db7373e122e2735faf0d97/tqdm-4.65.0-py3-none-any.whl (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 8.2MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/73/b094a662ae05cdc4ec95bc54e434e307986a5de5960166b8161b7c1373ee/filelock-3.12.0-py3-none-any.whl\n",
      "Collecting numpy>=1.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/d9/814a619ab84d8eb0d95e08d4c723e665f1e694b5a6068ca505a61bdc3745/numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 86.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/c3/57f0601a2d4fe15de7a553c00adbc901425661bf048f2a22dfc500caf121/packaging-23.1-py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.14.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/34/c57b951aecd0248845932c1cfc15721237c50e463f26b0536673bcb76f4f/huggingface_hub-0.14.1-py3-none-any.whl (224kB)\n",
      "\u001b[K     |████████████████████████████████| 225kB 101.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/80/034ffeca15c0f4e01b7b9c6ad0fb704b44e190cde4e757edbd60be404c41/requests-2.30.0-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 6.1MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/f2/017bf57106b845e31ef6179bf204042720a53629cf599ef9464da990e0e5/tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8MB 85.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/42/7ad4b6d67a16229496d4f6e74201bdbebcf4bc1e87d5a70c9297d4961bd2/PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701kB)\n",
      "\u001b[K     |████████████████████████████████| 706kB 89.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/7c/941e5c89bbbcd6ba460444c6ec029d54e7147741078f1c8300a8cbf8abb9/regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 99.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Downloading https://files.pythonhosted.org/packages/31/25/5abcd82372d3d4a3932e1fa8c3dbf9efac10cc7c0d16e78467460571b404/typing_extensions-4.5.0-py3-none-any.whl\n",
      "Collecting fsspec\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/4e/397b234a369df06ec782666fcdf9791d125ca6de48729814b381af8c6c03/fsspec-2023.5.0-py3-none-any.whl (160kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 108.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/34/3030de6f1370931b9dbb4dad48f6ab1015ab1d32447850b9fc94e60097be/idna-3.4-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 4.2MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/19/59961b522e6757f0c9097e4493fa906031b95b3ebe9360b2c3083561a6b4/certifi-2023.5.7-py3-none-any.whl (156kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 101.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/1d/f8383ef593114755429c307449e7717b87044b3bcd5f7860b89b1f759e34/urllib3-2.0.2-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 103.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/5f/361202de730532028458b729781b8435f320e31a622c27f30e25eec80513/charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 101.4MB/s eta 0:00:01\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.0.0a1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab-server 2.19.0 has requirement jsonschema>=4.17.3, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, filelock, numpy, packaging, typing-extensions, idna, certifi, urllib3, charset-normalizer, requests, pyyaml, fsspec, huggingface-hub, tokenizers, regex, transformers\n",
      "Successfully installed certifi-2023.5.7 charset-normalizer-3.1.0 filelock-3.12.0 fsspec-2023.5.0 huggingface-hub-0.14.1 idna-3.4 numpy-1.24.3 packaging-23.1 pyyaml-6.0 regex-2023.5.5 requests-2.30.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.29.2 typing-extensions-4.5.0 urllib3-2.0.2\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/typing_extensions.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/filelock already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna-3.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5713cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-2.0.1%2Bcpu-cp38-cp38-linux_x86_64.whl (195.4MB)\n",
      "\u001b[K     |████████████████████████████████| 195.4MB 32kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.15.2%2Bcpu-cp38-cp38-linux_x86_64.whl (24.9MB)\n",
      "\u001b[K     |████████████████████████████████| 24.9MB 102.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.0.2%2Bcpu-cp38-cp38-linux_x86_64.whl (4.1MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1MB 106.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/networkx-3.0-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 114.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Downloading https://download.pytorch.org/whl/typing_extensions-4.4.0-py3-none-any.whl\n",
      "Collecting filelock\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.9.0-py3-none-any.whl\n",
      "Collecting sympy\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/sympy-1.11.1-py3-none-any.whl (6.5MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5MB 97.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/Jinja2-3.1.2-py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 124.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 109.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/requests-2.28.1-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 3.6MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 105.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/mpmath-1.2.1-py3-none-any.whl (532kB)\n",
      "\u001b[K     |████████████████████████████████| 542kB 113.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/certifi-2022.12.7-py3-none-any.whl (155kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 111.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/urllib3-1.26.13-py2.py3-none-any.whl (140kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 114.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/idna-3.4-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 4.9MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
      "  Downloading https://download.pytorch.org/whl/charset_normalizer-2.1.1-py3-none-any.whl\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.0.0a1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab-server 2.19.0 has requirement jsonschema>=4.17.3, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: networkx, typing-extensions, filelock, mpmath, sympy, MarkupSafe, jinja2, torch, numpy, certifi, urllib3, idna, charset-normalizer, requests, pillow, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.2 certifi-2022.12.7 charset-normalizer-2.1.1 filelock-3.9.0 idna-3.4 jinja2-3.1.2 mpmath-1.2.1 networkx-3.0 numpy-1.24.1 pillow-9.3.0 requests-2.28.1 sympy-1.11.1 torch-2.0.1+cpu torchaudio-2.0.2+cpu torchvision-0.15.2+cpu typing-extensions-4.4.0 urllib3-1.26.13\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0c733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901aebd",
   "metadata": {},
   "source": [
    "## Use GPU (cuda device) if avalable to Train the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d50db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa559bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8df9d",
   "metadata": {},
   "source": [
    "## Custom dataset for summarization\n",
    "use following constructor to pass dataframe, tokenizer, maximum length of source text and maximum length of summary length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7df538b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.text\n",
    "        self.ctext = self.data.ctext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0992a20",
   "metadata": {},
   "source": [
    "## Creating the training function\n",
    " The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b655ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10 == 0:\n",
    "            print({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # xm.optimizer_step(optimizer)\n",
    "        # xm.mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8112fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f411d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba727125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "# tup = (\"hello\",\"54\",\"32cdsa\")\n",
    "# filename = '/data/artifacts/sample.pkl'\n",
    "# os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "# with open(filename, 'wb') as f:\n",
    "#     pickle.dump(tup, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be222a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/data/artifacts/sample.pkl', 'rb') as f:\n",
    "#      d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "885dc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training  \n",
    "TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "TRAIN_EPOCHS = 2       # number of epochs to train (default: 10)\n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 512\n",
    "SUMMARY_LEN = 150 \n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(SEED) # pytorch random seed\n",
    "np.random.seed(SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599f97c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602eb3781cf740d4b21e68215fc195e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e501e30561e9471caebdd40cd0240ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python/c3e3a6d1-48c5-4ec2-8739-9b9b511c7036/3.8/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd10f0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  The Administration of Union Territory Daman an...   \n",
      "1  Malaika Arora slammed an Instagram user who tr...   \n",
      "2  The Indira Gandhi Institute of Medical Science...   \n",
      "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
      "4  Hotels in Maharashtra will train their staff t...   \n",
      "\n",
      "                                               ctext  \n",
      "0  summarize: The Daman and Diu administration on...  \n",
      "1  summarize: From her special numbers to TV?appe...  \n",
      "2  summarize: The Indira Gandhi Institute of Medi...  \n",
      "3  summarize: Lashkar-e-Taiba's Kashmir commander...  \n",
      "4  summarize: Hotels in Mumbai and other Indian c...  \n"
     ]
    }
   ],
   "source": [
    "# Importing and Pre-Processing the domain data\n",
    "# Selecting the needed columns only. \n",
    "# Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "df = pd.read_csv('/data/news_summary.csv',encoding='latin-1')\n",
    "df = df[['text','ctext']]\n",
    "df.ctext = 'summarize: ' + df.ctext\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11816f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (4514, 2)\n",
      "TRAIN Dataset: (3611, 2)\n",
      "TEST Dataset: (903, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creation of Dataset and Dataloader\n",
    "# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state = SEED)\n",
    "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(val_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "900b9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Training and Validation dataset for further creation of Dataloader\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "val_params = {\n",
    "    'batch_size': VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d483ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e912441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model = model.to(device)\n",
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4482d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.9M\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  302 Apr 26 05:21 '=0.11.1'\r\n",
      "drwxrwxrwx 2 mosaic-ai mosaic-ai 4.0K Apr 26 05:21  GPT_pretrained\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai   13 Apr 26 05:21  README.md\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  97K Apr 26 05:32  T5.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  91K Apr 26 05:21  Untitled-Copy1.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai 1.6M Apr 26 05:21  Untitled.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  590 Apr 26 05:21  Untitled1.ipynb\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai  27K Apr 26 05:21  codeT5.ipynb\r\n",
      "drwxrwxrwx 3 mosaic-ai mosaic-ai 4.0K Apr 26 05:21  drive\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai   65 Apr 26 05:21  kaggle.json\r\n",
      "drwxrwxrwx 2 mosaic-ai mosaic-ai 4.0K Apr 26 05:21  smart-app\r\n",
      "-rwxrwxrwx 1 mosaic-ai mosaic-ai 4.0K Apr 26 05:21  yolopanda.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d111734",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/data/artifacts/pre-trained-T5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/artifacts/pre-trained-T5/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/data/artifacts/pre-trained-T5'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "filename = '/data/artifacts/pre-trained-T5/'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "tokenizer.save_pretrained(filename + \"tokenizer/\")\n",
    "model.save_pretrained(filename + 'model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f03e685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Fine-Tuning for the model on our dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python/c3e3a6d1-48c5-4ec2-8739-9b9b511c7036/3.8/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Training Loss': 8.058304786682129}\n",
      "Epoch: 0, Loss:  8.058304786682129\n",
      "{'Training Loss': 2.1611168384552}\n",
      "{'Training Loss': 3.1136369705200195}\n",
      "{'Training Loss': 1.859143614768982}\n",
      "{'Training Loss': 2.4855923652648926}\n",
      "{'Training Loss': 2.607058525085449}\n",
      "{'Training Loss': 3.0777788162231445}\n",
      "{'Training Loss': 2.5979037284851074}\n",
      "{'Training Loss': 1.6971368789672852}\n",
      "{'Training Loss': 2.125770330429077}\n",
      "{'Training Loss': 3.4051673412323}\n",
      "{'Training Loss': 2.5892767906188965}\n",
      "{'Training Loss': 1.6889514923095703}\n",
      "{'Training Loss': 2.7192437648773193}\n",
      "{'Training Loss': 2.3393096923828125}\n",
      "{'Training Loss': 1.426955223083496}\n",
      "{'Training Loss': 2.039098024368286}\n",
      "{'Training Loss': 1.9689128398895264}\n",
      "{'Training Loss': 2.7368674278259277}\n",
      "{'Training Loss': 2.252192258834839}\n",
      "{'Training Loss': 1.9333751201629639}\n",
      "{'Training Loss': 1.7331931591033936}\n",
      "{'Training Loss': 1.182373285293579}\n",
      "{'Training Loss': 2.7537801265716553}\n",
      "{'Training Loss': 1.0378023386001587}\n",
      "{'Training Loss': 0.9602898955345154}\n",
      "{'Training Loss': 3.5715901851654053}\n",
      "{'Training Loss': 2.243135929107666}\n",
      "{'Training Loss': 3.960209369659424}\n",
      "{'Training Loss': 1.7617555856704712}\n",
      "{'Training Loss': 1.6970088481903076}\n",
      "{'Training Loss': 1.4292314052581787}\n",
      "{'Training Loss': 2.286489486694336}\n",
      "{'Training Loss': 2.0775187015533447}\n",
      "{'Training Loss': 2.133932590484619}\n",
      "{'Training Loss': 4.345370769500732}\n",
      "{'Training Loss': 1.8192859888076782}\n",
      "{'Training Loss': 2.137387752532959}\n",
      "{'Training Loss': 0.9728744626045227}\n",
      "{'Training Loss': 0.8223398327827454}\n",
      "{'Training Loss': 1.883461833000183}\n",
      "{'Training Loss': 1.7700763940811157}\n",
      "{'Training Loss': 2.365732192993164}\n",
      "{'Training Loss': 1.873142123222351}\n",
      "{'Training Loss': 2.978618860244751}\n",
      "{'Training Loss': 2.004485607147217}\n",
      "{'Training Loss': 2.792855978012085}\n",
      "{'Training Loss': 1.7057881355285645}\n",
      "{'Training Loss': 1.2806766033172607}\n",
      "{'Training Loss': 1.718701958656311}\n",
      "{'Training Loss': 1.2089612483978271}\n",
      "Epoch: 0, Loss:  1.2089612483978271\n",
      "{'Training Loss': 2.809932231903076}\n",
      "{'Training Loss': 2.1601717472076416}\n",
      "{'Training Loss': 1.9173026084899902}\n",
      "{'Training Loss': 2.062079668045044}\n",
      "{'Training Loss': 2.03633713722229}\n",
      "{'Training Loss': 1.2346969842910767}\n",
      "{'Training Loss': 2.403790235519409}\n",
      "{'Training Loss': 1.9151242971420288}\n",
      "{'Training Loss': 1.04285728931427}\n",
      "{'Training Loss': 1.5964282751083374}\n",
      "{'Training Loss': 1.503917932510376}\n",
      "{'Training Loss': 0.9754344820976257}\n",
      "{'Training Loss': 2.570391893386841}\n",
      "{'Training Loss': 2.0042436122894287}\n",
      "{'Training Loss': 1.870619773864746}\n",
      "{'Training Loss': 1.219962477684021}\n",
      "{'Training Loss': 1.1904122829437256}\n",
      "{'Training Loss': 3.290990114212036}\n",
      "{'Training Loss': 1.7219845056533813}\n",
      "{'Training Loss': 1.4497567415237427}\n",
      "{'Training Loss': 2.72792387008667}\n",
      "{'Training Loss': 1.689603328704834}\n",
      "{'Training Loss': 1.2850282192230225}\n",
      "{'Training Loss': 2.2448198795318604}\n",
      "{'Training Loss': 2.4569175243377686}\n",
      "{'Training Loss': 2.145702362060547}\n",
      "{'Training Loss': 1.8851628303527832}\n",
      "{'Training Loss': 1.2703324556350708}\n",
      "{'Training Loss': 1.7852882146835327}\n",
      "{'Training Loss': 2.290771961212158}\n",
      "{'Training Loss': 1.8142688274383545}\n",
      "{'Training Loss': 1.776018500328064}\n",
      "{'Training Loss': 2.3263492584228516}\n",
      "{'Training Loss': 1.7726662158966064}\n",
      "{'Training Loss': 1.7775599956512451}\n",
      "{'Training Loss': 1.6760996580123901}\n",
      "{'Training Loss': 2.2457590103149414}\n",
      "{'Training Loss': 1.5167580842971802}\n",
      "{'Training Loss': 1.8733367919921875}\n",
      "{'Training Loss': 2.1604700088500977}\n",
      "{'Training Loss': 1.872895359992981}\n",
      "{'Training Loss': 2.0413882732391357}\n",
      "{'Training Loss': 1.8980019092559814}\n",
      "{'Training Loss': 1.7321077585220337}\n",
      "{'Training Loss': 1.7243845462799072}\n",
      "{'Training Loss': 1.0959831476211548}\n",
      "{'Training Loss': 1.856197714805603}\n",
      "{'Training Loss': 1.3174527883529663}\n",
      "{'Training Loss': 0.6950068473815918}\n",
      "{'Training Loss': 1.3992736339569092}\n",
      "Epoch: 0, Loss:  1.3992736339569092\n",
      "{'Training Loss': 1.5104058980941772}\n",
      "{'Training Loss': 1.4418903589248657}\n",
      "{'Training Loss': 3.4795162677764893}\n",
      "{'Training Loss': 1.972278118133545}\n",
      "{'Training Loss': 0.9514443874359131}\n",
      "{'Training Loss': 1.4983317852020264}\n",
      "{'Training Loss': 1.5233289003372192}\n",
      "{'Training Loss': 2.1414520740509033}\n",
      "{'Training Loss': 1.5707905292510986}\n",
      "{'Training Loss': 1.504554271697998}\n",
      "{'Training Loss': 1.8395918607711792}\n",
      "{'Training Loss': 1.294873595237732}\n",
      "{'Training Loss': 1.6670554876327515}\n",
      "{'Training Loss': 1.2999186515808105}\n",
      "{'Training Loss': 1.8954838514328003}\n",
      "{'Training Loss': 2.4904088973999023}\n",
      "{'Training Loss': 2.2206764221191406}\n",
      "{'Training Loss': 2.3650407791137695}\n",
      "{'Training Loss': 1.0525134801864624}\n",
      "{'Training Loss': 2.4119277000427246}\n",
      "{'Training Loss': 0.915443480014801}\n",
      "{'Training Loss': 2.2052040100097656}\n",
      "{'Training Loss': 1.0773909091949463}\n",
      "{'Training Loss': 1.5961641073226929}\n",
      "{'Training Loss': 2.0363123416900635}\n",
      "{'Training Loss': 0.9914813041687012}\n",
      "{'Training Loss': 1.5119420289993286}\n",
      "{'Training Loss': 1.8900800943374634}\n",
      "{'Training Loss': 1.8629223108291626}\n",
      "{'Training Loss': 1.1743500232696533}\n",
      "{'Training Loss': 1.530099630355835}\n",
      "{'Training Loss': 1.4479632377624512}\n",
      "{'Training Loss': 1.4297642707824707}\n",
      "{'Training Loss': 1.9563044309616089}\n",
      "{'Training Loss': 1.9886047840118408}\n",
      "{'Training Loss': 1.4962059259414673}\n",
      "{'Training Loss': 1.282868504524231}\n",
      "{'Training Loss': 1.7532585859298706}\n",
      "{'Training Loss': 3.1929948329925537}\n",
      "{'Training Loss': 1.0125713348388672}\n",
      "{'Training Loss': 1.6768397092819214}\n",
      "{'Training Loss': 1.1373565196990967}\n",
      "{'Training Loss': 1.874818205833435}\n",
      "{'Training Loss': 1.6799763441085815}\n",
      "{'Training Loss': 1.2442892789840698}\n",
      "{'Training Loss': 1.79205322265625}\n",
      "{'Training Loss': 1.452300786972046}\n",
      "{'Training Loss': 1.3909493684768677}\n",
      "{'Training Loss': 2.545313596725464}\n",
      "{'Training Loss': 1.0124331712722778}\n",
      "Epoch: 0, Loss:  1.0124331712722778\n",
      "{'Training Loss': 1.84011971950531}\n",
      "{'Training Loss': 1.383560061454773}\n",
      "{'Training Loss': 1.813107967376709}\n",
      "{'Training Loss': 2.073561906814575}\n",
      "{'Training Loss': 2.757591724395752}\n",
      "{'Training Loss': 3.140136957168579}\n",
      "{'Training Loss': 1.4678598642349243}\n",
      "{'Training Loss': 2.179044485092163}\n",
      "{'Training Loss': 2.1863653659820557}\n",
      "{'Training Loss': 1.843891978263855}\n",
      "{'Training Loss': 1.839512825012207}\n",
      "{'Training Loss': 1.6883225440979004}\n",
      "{'Training Loss': 2.5314011573791504}\n",
      "{'Training Loss': 1.4900212287902832}\n",
      "{'Training Loss': 2.234158515930176}\n",
      "{'Training Loss': 2.1548469066619873}\n",
      "{'Training Loss': 1.6055132150650024}\n",
      "{'Training Loss': 1.8626631498336792}\n",
      "{'Training Loss': 2.315223455429077}\n",
      "{'Training Loss': 1.6464273929595947}\n",
      "{'Training Loss': 1.2810866832733154}\n",
      "{'Training Loss': 1.8960416316986084}\n",
      "{'Training Loss': 2.0504050254821777}\n",
      "{'Training Loss': 1.2872530221939087}\n",
      "{'Training Loss': 1.4512531757354736}\n",
      "{'Training Loss': 1.35677170753479}\n",
      "{'Training Loss': 2.0707590579986572}\n",
      "{'Training Loss': 1.6203663349151611}\n",
      "{'Training Loss': 2.157012701034546}\n",
      "{'Training Loss': 2.1295785903930664}\n",
      "{'Training Loss': 1.1913293600082397}\n",
      "Epoch: 1, Loss:  1.1913293600082397\n",
      "{'Training Loss': 1.6279257535934448}\n",
      "{'Training Loss': 1.0137428045272827}\n",
      "{'Training Loss': 1.2573895454406738}\n",
      "{'Training Loss': 1.2423242330551147}\n",
      "{'Training Loss': 1.294519305229187}\n",
      "{'Training Loss': 1.0451101064682007}\n",
      "{'Training Loss': 1.3656421899795532}\n",
      "{'Training Loss': 1.7283040285110474}\n",
      "{'Training Loss': 1.7634717226028442}\n",
      "{'Training Loss': 2.805511474609375}\n",
      "{'Training Loss': 1.445009469985962}\n",
      "{'Training Loss': 1.240993618965149}\n",
      "{'Training Loss': 1.726074457168579}\n",
      "{'Training Loss': 1.118496298789978}\n",
      "{'Training Loss': 0.9418358206748962}\n",
      "{'Training Loss': 2.126141309738159}\n",
      "{'Training Loss': 1.029618263244629}\n",
      "{'Training Loss': 1.3045028448104858}\n",
      "{'Training Loss': 2.2856454849243164}\n",
      "{'Training Loss': 1.502455472946167}\n",
      "{'Training Loss': 0.7864449620246887}\n",
      "{'Training Loss': 2.0497264862060547}\n",
      "{'Training Loss': 1.427481770515442}\n",
      "{'Training Loss': 1.1077769994735718}\n",
      "{'Training Loss': 0.8359091281890869}\n",
      "{'Training Loss': 1.460636019706726}\n",
      "{'Training Loss': 1.0313453674316406}\n",
      "{'Training Loss': 2.305976390838623}\n",
      "{'Training Loss': 0.6257269978523254}\n",
      "{'Training Loss': 2.185368299484253}\n",
      "{'Training Loss': 1.5313048362731934}\n",
      "{'Training Loss': 1.8155441284179688}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Training Loss': 1.325231671333313}\n",
      "{'Training Loss': 1.568060278892517}\n",
      "{'Training Loss': 1.0346990823745728}\n",
      "{'Training Loss': 1.2913224697113037}\n",
      "{'Training Loss': 1.017831802368164}\n",
      "{'Training Loss': 2.127565383911133}\n",
      "{'Training Loss': 1.5928393602371216}\n",
      "{'Training Loss': 1.4604525566101074}\n",
      "{'Training Loss': 1.3353371620178223}\n",
      "{'Training Loss': 1.9908702373504639}\n",
      "{'Training Loss': 1.0247286558151245}\n",
      "{'Training Loss': 2.513697862625122}\n",
      "{'Training Loss': 1.639871597290039}\n",
      "{'Training Loss': 1.595216155052185}\n",
      "{'Training Loss': 1.6392605304718018}\n",
      "{'Training Loss': 0.9692506194114685}\n",
      "{'Training Loss': 1.208333969116211}\n",
      "{'Training Loss': 1.07382333278656}\n",
      "Epoch: 1, Loss:  1.07382333278656\n",
      "{'Training Loss': 1.3480194807052612}\n",
      "{'Training Loss': 2.035156488418579}\n",
      "{'Training Loss': 0.9873034358024597}\n",
      "{'Training Loss': 1.032651424407959}\n",
      "{'Training Loss': 0.8308926224708557}\n",
      "{'Training Loss': 1.8320159912109375}\n",
      "{'Training Loss': 1.2252055406570435}\n",
      "{'Training Loss': 2.8879380226135254}\n",
      "{'Training Loss': 0.9493544697761536}\n",
      "{'Training Loss': 1.9403470754623413}\n",
      "{'Training Loss': 2.2437376976013184}\n",
      "{'Training Loss': 1.4575130939483643}\n",
      "{'Training Loss': 1.6811083555221558}\n",
      "{'Training Loss': 1.7276763916015625}\n",
      "{'Training Loss': 1.760022759437561}\n",
      "{'Training Loss': 1.804746150970459}\n",
      "{'Training Loss': 1.206840991973877}\n",
      "{'Training Loss': 2.4944233894348145}\n",
      "{'Training Loss': 1.4594494104385376}\n",
      "{'Training Loss': 1.9027186632156372}\n",
      "{'Training Loss': 2.706540822982788}\n",
      "{'Training Loss': 0.8425273299217224}\n",
      "{'Training Loss': 1.8517589569091797}\n",
      "{'Training Loss': 0.793498158454895}\n",
      "{'Training Loss': 0.9966477751731873}\n",
      "{'Training Loss': 1.545805811882019}\n",
      "{'Training Loss': 1.3770540952682495}\n",
      "{'Training Loss': 1.656991958618164}\n",
      "{'Training Loss': 1.0883722305297852}\n",
      "{'Training Loss': 1.8430426120758057}\n",
      "{'Training Loss': 1.719552993774414}\n",
      "{'Training Loss': 1.4328970909118652}\n",
      "{'Training Loss': 1.0393342971801758}\n",
      "{'Training Loss': 1.5362977981567383}\n",
      "{'Training Loss': 1.1907085180282593}\n",
      "{'Training Loss': 1.1387014389038086}\n",
      "{'Training Loss': 1.9741015434265137}\n",
      "{'Training Loss': 1.1995413303375244}\n",
      "{'Training Loss': 2.008927822113037}\n",
      "{'Training Loss': 1.4487658739089966}\n",
      "{'Training Loss': 1.8012644052505493}\n",
      "{'Training Loss': 1.108107328414917}\n",
      "{'Training Loss': 1.3700027465820312}\n",
      "{'Training Loss': 1.7188847064971924}\n",
      "{'Training Loss': 1.6383357048034668}\n",
      "{'Training Loss': 1.117260456085205}\n",
      "{'Training Loss': 1.021905541419983}\n",
      "{'Training Loss': 1.688132405281067}\n",
      "{'Training Loss': 1.9032294750213623}\n",
      "{'Training Loss': 1.8542094230651855}\n",
      "Epoch: 1, Loss:  1.8542094230651855\n",
      "{'Training Loss': 1.4478914737701416}\n",
      "{'Training Loss': 1.2455583810806274}\n",
      "{'Training Loss': 1.5166306495666504}\n",
      "{'Training Loss': 1.0541797876358032}\n",
      "{'Training Loss': 1.4590392112731934}\n",
      "{'Training Loss': 1.4593913555145264}\n",
      "{'Training Loss': 1.688456416130066}\n",
      "{'Training Loss': 2.3579585552215576}\n",
      "{'Training Loss': 1.185002088546753}\n",
      "{'Training Loss': 1.0803073644638062}\n",
      "{'Training Loss': 1.9543694257736206}\n",
      "{'Training Loss': 1.6498502492904663}\n",
      "{'Training Loss': 1.8167896270751953}\n",
      "{'Training Loss': 1.3522733449935913}\n",
      "{'Training Loss': 1.5139002799987793}\n",
      "{'Training Loss': 1.0732991695404053}\n",
      "{'Training Loss': 3.176067590713501}\n",
      "{'Training Loss': 0.9136605858802795}\n",
      "{'Training Loss': 1.2827496528625488}\n",
      "{'Training Loss': 1.2477184534072876}\n",
      "{'Training Loss': 1.7420775890350342}\n",
      "{'Training Loss': 2.02280330657959}\n",
      "{'Training Loss': 1.4692177772521973}\n",
      "{'Training Loss': 2.373905658721924}\n",
      "{'Training Loss': 1.0662583112716675}\n",
      "{'Training Loss': 1.488287091255188}\n",
      "{'Training Loss': 1.8575632572174072}\n",
      "{'Training Loss': 1.393693447113037}\n",
      "{'Training Loss': 1.219125747680664}\n",
      "{'Training Loss': 1.9220623970031738}\n",
      "{'Training Loss': 1.8151379823684692}\n",
      "{'Training Loss': 1.2178592681884766}\n",
      "{'Training Loss': 2.5144472122192383}\n",
      "{'Training Loss': 1.825502872467041}\n",
      "{'Training Loss': 1.8170185089111328}\n",
      "{'Training Loss': 1.4524534940719604}\n",
      "{'Training Loss': 1.5483006238937378}\n",
      "{'Training Loss': 1.5331557989120483}\n",
      "{'Training Loss': 1.0640233755111694}\n",
      "{'Training Loss': 1.7108306884765625}\n",
      "{'Training Loss': 1.7477190494537354}\n",
      "{'Training Loss': 1.2429012060165405}\n",
      "{'Training Loss': 1.4206604957580566}\n",
      "{'Training Loss': 1.4286260604858398}\n",
      "{'Training Loss': 1.7950655221939087}\n",
      "{'Training Loss': 1.3782341480255127}\n",
      "{'Training Loss': 1.3116986751556396}\n",
      "{'Training Loss': 1.147565245628357}\n",
      "{'Training Loss': 1.3271093368530273}\n",
      "{'Training Loss': 1.9252617359161377}\n",
      "Epoch: 1, Loss:  1.9252617359161377\n",
      "{'Training Loss': 2.2396697998046875}\n",
      "{'Training Loss': 1.1837023496627808}\n",
      "{'Training Loss': 0.9832320213317871}\n",
      "{'Training Loss': 1.2273029088974}\n",
      "{'Training Loss': 1.8068865537643433}\n",
      "{'Training Loss': 1.5595779418945312}\n",
      "{'Training Loss': 1.064821720123291}\n",
      "{'Training Loss': 1.1838206052780151}\n",
      "{'Training Loss': 1.4736393690109253}\n",
      "{'Training Loss': 1.4368454217910767}\n",
      "{'Training Loss': 1.696932077407837}\n",
      "{'Training Loss': 2.1057662963867188}\n",
      "{'Training Loss': 1.233380913734436}\n",
      "{'Training Loss': 1.549364447593689}\n",
      "{'Training Loss': 1.3434139490127563}\n",
      "{'Training Loss': 2.211829900741577}\n",
      "{'Training Loss': 1.6031014919281006}\n",
      "{'Training Loss': 1.4400289058685303}\n",
      "{'Training Loss': 2.0239648818969727}\n",
      "{'Training Loss': 2.163097858428955}\n",
      "{'Training Loss': 1.721250057220459}\n",
      "{'Training Loss': 1.2847744226455688}\n",
      "{'Training Loss': 1.339850902557373}\n",
      "{'Training Loss': 1.1902973651885986}\n",
      "{'Training Loss': 1.6479030847549438}\n",
      "{'Training Loss': 1.272547721862793}\n",
      "{'Training Loss': 1.5471971035003662}\n",
      "{'Training Loss': 1.9258514642715454}\n",
      "{'Training Loss': 2.2941977977752686}\n",
      "{'Training Loss': 1.4419958591461182}\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print('Initiating Fine-Tuning for the model on our dataset')\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c550527",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = (tokenizer, model, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6f77982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "filename = '/data/artifacts/training_artifacts_T5_news1.pkl'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(artifacts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99009a7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary +: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGood morning... hope the code has run successfully... the time training was started : 2023-04-02 19:19:22. And the time it ended : \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary +: 'str'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(\"Good morning... hope the code has run successfully... the time training was started : 2023-04-02 19:19:22. And the time it ended : \"+ str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89a3bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11930fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# !ls /home/mosaic-ai/.cache/huggingface/hub/models--t5-base/snapshots/\n",
    "!mv /home/mosaic-ai/.cache/huggingface/hub/models--t5-base/snapshots/fe6d9bf207cd3337512ca838a8b453f87a9178ef /home/mosaic-ai/.cache/huggingface/hub/models--t5-base/snapshots/0db7e623bcaee2daf9b859a646637ea39bf016cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd33f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_loaded = T5ForConditionalGeneration.from_pretrained(\"/data/artifacts/model/\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/data/artifacts/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af82c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "filename = '/data/artifacts/training_artifacts_T5_news.pkl'\n",
    "artifacts = pickle.load(open(filename, 'rb'))\n",
    "print(type(artifacts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3c109b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(T5Tokenizer(name_or_path='t5-base', vocab_size=32100, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}),\n",
       " T5ForConditionalGeneration(\n",
       "   (shared): Embedding(32128, 768)\n",
       "   (encoder): T5Stack(\n",
       "     (embed_tokens): Embedding(32128, 768)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 12)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1-11): 11 x T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (decoder): T5Stack(\n",
       "     (embed_tokens): Embedding(32128, 768)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 12)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1-11): 11 x T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "               (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseActDense(\n",
       "               (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "               (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (act): ReLU()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       " ),\n",
       " 'cpu',\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f0d3fcd99d0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b1dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# artifacts = (tokenizer, model, device, val_loader)\n",
    "tokenizer = artifacts[0]\n",
    "model = artifacts[1]\n",
    "device = artifacts[2]\n",
    "val_loader = artifacts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f35073",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"/data/artifacts/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/data/artifacts/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python/c3e3a6d1-48c5-4ec2-8739-9b9b511c7036/3.8/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0\n",
      "Completed 100\n"
     ]
    }
   ],
   "source": [
    "# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "# Saving the dataframe as predictions.csv\n",
    "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "for epoch in range(VAL_EPOCHS):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    final_df.to_csv('/data/artifacts/predictions_artifacts_T5_news.csv')\n",
    "    print('Output Files generated for review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd43a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod= (tokenizer,model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79bffa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7813794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scoring_func\n",
    "def score(model, request):\n",
    "    print(\"++++++++++++++++++---In Score Function---++++++++++++++++++++++\")\n",
    "    import pickle\n",
    "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "    model_loaded = T5ForConditionalGeneration.from_pretrained(\"/data/artifacts/model/\")\n",
    "    print(\"++++++++++++++++++---Fine-Tuned Model Loaded---++++++++++++++++++++++\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"/data/artifacts/tokenizer/\")\n",
    "    print(\"++++++++++++++++++---Tokenizer Loaded---++++++++++++++++++++++\")\n",
    "    text = request.json[\"payload\"]\n",
    "#     tokenizer = model[0]\n",
    "#     mod = model[1]\n",
    "    print(\"++++++++++++++++++---Input Text Below---++++++++++++++++++++++\")\n",
    "    print(text)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    mod = model_loaded\n",
    "    source = tokenizer.batch_encode_plus([text], max_length=512, pad_to_max_length=True,return_tensors='pt')\n",
    "    source_ids = source['input_ids']\n",
    "    source_mask = source['attention_mask']\n",
    "    print(\"++++++++++++++++++---Input Tokenized---++++++++++++++++++++++\")\n",
    "    generated_ids = mod.generate(\n",
    "        input_ids = source_ids,\n",
    "        attention_mask = source_mask, \n",
    "        max_length=150, \n",
    "        num_beams=2,\n",
    "        repetition_penalty=2.5, \n",
    "        length_penalty=1.0, \n",
    "        early_stopping=True\n",
    "        \n",
    "    )\n",
    "    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "    print(\"++++++++++++++++++---Prediction Done, Ouput Below---++++++++++++++++++++++\")\n",
    "    \n",
    "    print(preds)\n",
    "    \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cfaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "req = requests.Request()\n",
    "req.json = {\"payload\":\"\"\"\"\n",
    "New York City Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic.\n",
    "\n",
    "During a press conference on Thursday, de Blasio said that the city will lift all restrictions on businesses, restaurants, and other venues starting on July 1st, as long as the vaccination rates continue to climb and the number of new COVID-19 cases remains low.\n",
    "\n",
    "\"This is going to be the summer of New York City,\" de Blasio said. \"We are ready to bring back the life and energy that we all know and love in this city.\"\n",
    "\n",
    "New York City was once the epicenter of the COVID-19 pandemic in the United States, with tens of thousands of cases and deaths in the early months of the outbreak. But the city has since made tremendous progress in controlling the spread of the virus, thanks in large part to an aggressive vaccination campaign that has already reached more than 6 million doses administered.\n",
    "\n",
    "The city's seven-day average positivity rate is currently at 3.18%, down from a high of 7.39% in early January. The number of hospitalizations has also dropped significantly, with just over 2,000 patients currently being treated for COVID-19, down from a peak of more than 18,000 in April 2020.\n",
    "\n",
    "The news of the city's reopening plan was met with enthusiasm from residents and business owners alike, many of whom have struggled to make ends meet during the pandemic.\n",
    "\n",
    "\"We've been waiting for this day for so long,\" said Lisa Ruiz, the owner of a small restaurant in Brooklyn. \"It's been a tough year for us, but we're ready to come back stronger than ever.\"\n",
    "\n",
    "However, some public health experts have expressed caution about the city's plans to fully reopen, noting that the virus is still present in the community and that new variants could pose a threat in the future.\n",
    "\n",
    "\"It's important that we continue to monitor the situation closely and be prepared to take action if necessary,\" said Dr. Jay Varma, the city's senior health advisor.\n",
    "\n",
    "Despite these concerns, de Blasio said that he was confident that the city's vaccination efforts and other safety measures would be enough to prevent any major outbreaks.\n",
    "\n",
    "\"We're not going to take any chances, we're going to make sure that we're following the science and the data,\" he said. \"But I believe that we can do this safely and responsibly, and we're going to make it happen.\"\n",
    "\n",
    "The announcement of New York City's reopening plan comes as other parts of the country are also beginning to lift restrictions and return to normal. California, for example, announced last month that it would fully reopen on June 15th, while other states have already lifted mask mandates and capacity limits on businesses.\n",
    "\n",
    "For many people, the news of the city's reopening is a sign of hope and optimism after a long and difficult year.\n",
    "\n",
    "\"I can't wait to go to concerts, restaurants, and all the other things that make New York City so special,\" said Paul Davis, a resident of the city. It's been a tough year, but this gives us something to look forward to.\"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99b6e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++---In Score Function---++++++++++++++++++++++\n",
      "++++++++++++++++++---Fine-Tuned Model Loaded---++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++---Tokenizer Loaded---++++++++++++++++++++++\n",
      "++++++++++++++++++---Input Text Below---++++++++++++++++++++++\n",
      "\"\n",
      "New York City Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic.\n",
      "\n",
      "During a press conference on Thursday, de Blasio said that the city will lift all restrictions on businesses, restaurants, and other venues starting on July 1st, as long as the vaccination rates continue to climb and the number of new COVID-19 cases remains low.\n",
      "\n",
      "\"This is going to be the summer of New York City,\" de Blasio said. \"We are ready to bring back the life and energy that we all know and love in this city.\"\n",
      "\n",
      "New York City was once the epicenter of the COVID-19 pandemic in the United States, with tens of thousands of cases and deaths in the early months of the outbreak. But the city has since made tremendous progress in controlling the spread of the virus, thanks in large part to an aggressive vaccination campaign that has already reached more than 6 million doses administered.\n",
      "\n",
      "The city's seven-day average positivity rate is currently at 3.18%, down from a high of 7.39% in early January. The number of hospitalizations has also dropped significantly, with just over 2,000 patients currently being treated for COVID-19, down from a peak of more than 18,000 in April 2020.\n",
      "\n",
      "The news of the city's reopening plan was met with enthusiasm from residents and business owners alike, many of whom have struggled to make ends meet during the pandemic.\n",
      "\n",
      "\"We've been waiting for this day for so long,\" said Lisa Ruiz, the owner of a small restaurant in Brooklyn. \"It's been a tough year for us, but we're ready to come back stronger than ever.\"\n",
      "\n",
      "However, some public health experts have expressed caution about the city's plans to fully reopen, noting that the virus is still present in the community and that new variants could pose a threat in the future.\n",
      "\n",
      "\"It's important that we continue to monitor the situation closely and be prepared to take action if necessary,\" said Dr. Jay Varma, the city's senior health advisor.\n",
      "\n",
      "Despite these concerns, de Blasio said that he was confident that the city's vaccination efforts and other safety measures would be enough to prevent any major outbreaks.\n",
      "\n",
      "\"We're not going to take any chances, we're going to make sure that we're following the science and the data,\" he said. \"But I believe that we can do this safely and responsibly, and we're going to make it happen.\"\n",
      "\n",
      "The announcement of New York City's reopening plan comes as other parts of the country are also beginning to lift restrictions and return to normal. California, for example, announced last month that it would fully reopen on June 15th, while other states have already lifted mask mandates and capacity limits on businesses.\n",
      "\n",
      "For many people, the news of the city's reopening is a sign of hope and optimism after a long and difficult year.\n",
      "\n",
      "\"I can't wait to go to concerts, restaurants, and all the other things that make New York City so special,\" said Paul Davis, a resident of the city. It's been a tough year, but this gives us something to look forward to.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++---Input Tokenized---++++++++++++++++++++++\n",
      "++++++++++++++++++---Prediction Done, Ouput Below---++++++++++++++++++++++\n",
      "['Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \"We are ready to bring back the life and energy that we all know and love in this city,\" said Lisa Ruiz, the owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city\\'s plans to fully reopen.']\n"
     ]
    }
   ],
   "source": [
    "response = score(None,req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a42ebcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \"We are ready to bring back the life and energy that we all know and love in this city,\" said Lisa Ruiz, the owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city\\'s plans to fully reopen.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6758f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8babc4679a54b2bbee60b756ed1fe69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<style>.grad_1{background: #2468a4;} .grad_2{ color:white; background: #2468a4;}</s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "register_model((model,tokenizer),\n",
    "               score,\n",
    "               \"T5_Summarization_v3\",\n",
    "               \"T5_Summarization\",\n",
    "               MLModelFlavours.sklearn,\n",
    "               init_script=\"pip install SentencePiece \\\\n pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a78188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting refractml\n",
      "  Using cached https://files.pythonhosted.org/packages/37/19/a4e26eaa2ef35252149e815df6766634185b4f8630e54c357b20cfae4ec9/refractml-1.0.2-py2.py3-none-any.whl\n",
      "Processing /home/mosaic-ai/.cache/pip/wheels/ab/d0/0e/613976a1b51b5654859e2a82ade64329859bce431e280f2a39/shutils-0.1.0-cp38-none-any.whl\n",
      "Collecting cloudpickle==1.6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
      "Collecting requests-toolbelt==0.9.1\n",
      "  Using cached https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl\n",
      "Collecting PyYAML==6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/42/7ad4b6d67a16229496d4f6e74201bdbebcf4bc1e87d5a70c9297d4961bd2/PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Collecting mosaic-utils==1.0.2\n",
      "  Using cached https://files.pythonhosted.org/packages/09/d7/8424b1dcaa5b1a2f824fc440aa1c4ef45e0bf6593d11b37962311614f365/mosaic_utils-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pymysql\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/b1/bb485db528749f07d6f11aa123e5f931f2e465a9c27945d6122bae5f7df7/PyMySQL-1.0.3-py3-none-any.whl\n",
      "Collecting configparser\n",
      "  Using cached https://files.pythonhosted.org/packages/e0/7a/9d0f52bf4923b2e410c7d6fda472c32d9b728284e89ec99074820226102f/configparser-5.3.0-py3-none-any.whl\n",
      "Collecting requests<3.0.0,>=2.0.1\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/f4/274d1dbe96b41cf4e0efb70cbced278ffd61b5c7bb70338b62af94ccb25b/requests-2.28.2-py3-none-any.whl\n",
      "Collecting scikit-learn==1.2.1; python_version >= \"3.8\"\n",
      "  Using cached https://files.pythonhosted.org/packages/f0/95/0ea0a2412e33080a47ec02802210c008a7a540471581c95145f030d304b4/scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached https://files.pythonhosted.org/packages/74/5f/361202de730532028458b729781b8435f320e31a622c27f30e25eec80513/charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/34/3030de6f1370931b9dbb4dad48f6ab1015ab1d32447850b9fc94e60097be/idna-3.4-py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached https://files.pythonhosted.org/packages/71/4c/3db2b8021bd6f2f0ceb0e088d6b2d49147671f25832fb17970e9b583d742/certifi-2022.12.7-py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/f5/890a0baca17a61c1f92f72b81d3c31523c99bec609e60c292ea55b387ae8/urllib3-1.26.15-py2.py3-none-any.whl\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached https://files.pythonhosted.org/packages/61/cf/6e354304bcb9c6413c4e02a747b600061c21d38ba51e7e544ac7bc66aecc/threadpoolctl-3.1.0-py3-none-any.whl\n",
      "Collecting scipy>=1.3.2\n",
      "  Using cached https://files.pythonhosted.org/packages/69/f0/fb07a9548e48b687b8bf2fa81d71aba9cfc548d365046ca1c791e24db99d/scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting numpy>=1.17.3\n",
      "  Using cached https://files.pythonhosted.org/packages/8b/d9/814a619ab84d8eb0d95e08d4c723e665f1e694b5a6068ca505a61bdc3745/numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached https://files.pythonhosted.org/packages/91/d4/3b4c8e5a30604df4c7518c562d4bf0502f2fa29221459226e140cf846512/joblib-1.2.0-py3-none-any.whl\n",
      "\u001b[31mERROR: jupyterlab-server 2.19.0 has requirement jsonschema>=4.17.3, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pymysql, configparser, shutils, cloudpickle, charset-normalizer, idna, certifi, urllib3, requests, requests-toolbelt, PyYAML, threadpoolctl, numpy, scipy, joblib, scikit-learn, mosaic-utils, refractml\n",
      "Successfully installed PyYAML-6.0 certifi-2022.12.7 charset-normalizer-3.1.0 cloudpickle-1.6.0 configparser-5.3.0 idna-3.4 joblib-1.2.0 mosaic-utils-1.0.2 numpy-1.24.3 pymysql-1.0.3 refractml-1.0.2 requests-2.28.2 requests-toolbelt-0.9.1 scikit-learn-1.2.1 scipy-1.10.1 shutils-0.1.0 threadpoolctl-3.1.0 urllib3-1.26.15\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/PyYAML-6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaicml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/backports already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy-1.10.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/cloudpickle-1.6.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests-2.28.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/certifi-2022.12.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/refractml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy-1.24.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3-1.26.15.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer-3.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaic_utils already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/refractml-1.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/_yaml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests_toolbelt already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/yaml already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/requests_toolbelt-0.9.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib-1.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/PyMySQL-1.0.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/mosaic_utils-1.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/shutils already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl-3.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn-1.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tests already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/charset_normalizer already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/cloudpickle already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/idna-3.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/configparser-5.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/configparser.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pymysql already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/shutils-0.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 23.1.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install refractml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcdadf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'payload': \"New York City Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. During a press conference on Thursday, de Blasio said that the city will lift all restrictions on businesses, restaurants, and other venues starting on July 1st, as long as the vaccination rates continue to climb and the number of new COVID-19 cases remains low. 'This is going to be the summer of New York City,' de Blasio said. 'We are ready to bring back the life and energy that we all know and love in this city.' New York City was once the epicenter of the COVID-19 pandemic in the United States, with tens of thousands of cases and deaths in the early months of the outbreak. But the city has since made tremendous progress in controlling the spread of the virus, thanks in large part to an aggressive vaccination campaign that has already reached more than 6 million doses administered. The city's seven-day average positivity rate is currently at 3.18%, down from a high of 7.39% in early January. The number of hospitalizations has also dropped significantly, with just over 2,000 patients currently being treated for COVID-19, down from a peak of more than 18,000 in April 2020. The news of the city's reopening plan was met with enthusiasm from residents and business owners alike, many of whom have struggled to make ends meet during the pandemic. 'We have been waiting for this day for so long,' said Lisa Ruiz, the owner of a small restaurant in Brooklyn. 'It has been a tough year for us, but we're ready to come back stronger than ever.' However, some public health experts have expressed caution about the city's plans to fully reopen, noting that the virus is still present in the community and that new variants could pose a threat in the future. 'It is important that we continue to monitor the situation closely and be prepared to take action if necessary,i said Dr. Jay Varma, the city's senior health advisor. Despite these concerns, de Blasio said that he was confident that the city's vaccination efforts and other safety measures would be enough to prevent any major outbreaks. 'We are not going to take any chances, we're going to make sure that we're following the science and the data,' he said. 'But I believe that we can do this safely and responsibly, and we're going to make it happen.' The announcement of New York City's reopening plan comes as other parts of the country are also beginning to lift restrictions and return to normal. California, for example, announced last month that it would fully reopen on June 15th, while other states have already lifted mask mandates and capacity limits on businesses. For many people, the news of the city's reopening is a sign of hope and optimism after a long and difficult year. 'I can't wait to go to concerts, restaurants, and all the other things that make New York City so special,' said Paul Davis, a resident of the city. It's been a tough year, but this gives us something to look forward to.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"payload\":\"New York City Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. During a press conference on Thursday, de Blasio said that the city will lift all restrictions on businesses, restaurants, and other venues starting on July 1st, as long as the vaccination rates continue to climb and the number of new COVID-19 cases remains low. 'This is going to be the summer of New York City,' de Blasio said. 'We are ready to bring back the life and energy that we all know and love in this city.' New York City was once the epicenter of the COVID-19 pandemic in the United States, with tens of thousands of cases and deaths in the early months of the outbreak. But the city has since made tremendous progress in controlling the spread of the virus, thanks in large part to an aggressive vaccination campaign that has already reached more than 6 million doses administered. The city's seven-day average positivity rate is currently at 3.18%, down from a high of 7.39% in early January. The number of hospitalizations has also dropped significantly, with just over 2,000 patients currently being treated for COVID-19, down from a peak of more than 18,000 in April 2020. The news of the city's reopening plan was met with enthusiasm from residents and business owners alike, many of whom have struggled to make ends meet during the pandemic. 'We have been waiting for this day for so long,' said Lisa Ruiz, the owner of a small restaurant in Brooklyn. 'It has been a tough year for us, but we're ready to come back stronger than ever.' However, some public health experts have expressed caution about the city's plans to fully reopen, noting that the virus is still present in the community and that new variants could pose a threat in the future. 'It is important that we continue to monitor the situation closely and be prepared to take action if necessary,i said Dr. Jay Varma, the city's senior health advisor. Despite these concerns, de Blasio said that he was confident that the city's vaccination efforts and other safety measures would be enough to prevent any major outbreaks. 'We are not going to take any chances, we're going to make sure that we're following the science and the data,' he said. 'But I believe that we can do this safely and responsibly, and we're going to make it happen.' The announcement of New York City's reopening plan comes as other parts of the country are also beginning to lift restrictions and return to normal. California, for example, announced last month that it would fully reopen on June 15th, while other states have already lifted mask mandates and capacity limits on businesses. For many people, the news of the city's reopening is a sign of hope and optimism after a long and difficult year. 'I can't wait to go to concerts, restaurants, and all the other things that make New York City so special,' said Paul Davis, a resident of the city. It's been a tough year, but this gives us something to look forward to.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ce6fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': ['Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \"We are ready to bring back the life and energy that we all know and love in this city,\" said an owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city\\'s plans to fully reopen.'],\n",
       " 'request_id': 'b38f7391-32fc-4a3b-99c6-bef4338d9671',\n",
       " 'upload_logging_data': {'end_time': '2023-04-24 05:30:00.963129',\n",
       "  'feedback': '',\n",
       "  'model_id': '636f1810-7f82-4cc5-81e4-c6ae4b0ffedc',\n",
       "  'request_id': 'b38f7391-32fc-4a3b-99c6-bef4338d9671',\n",
       "  'response_data': ['Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \"We are ready to bring back the life and energy that we all know and love in this city,\" said an owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city\\'s plans to fully reopen.'],\n",
       "  'start_time': '2023-04-24 05:29:30.218434',\n",
       "  'status': 'Success',\n",
       "  'status_code': 200,\n",
       "  'version_id': '7e3864cc-852f-47de-b677-bd3251d46715'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"data\":[\"Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \\\"We are ready to bring back the life and energy that we all know and love in this city,\\\" said an owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city's plans to fully reopen.\"],\"request_id\":\"b38f7391-32fc-4a3b-99c6-bef4338d9671\",\"upload_logging_data\":{\"end_time\":\"2023-04-24 05:30:00.963129\",\"feedback\":\"\",\"model_id\":\"636f1810-7f82-4cc5-81e4-c6ae4b0ffedc\",\"request_id\":\"b38f7391-32fc-4a3b-99c6-bef4338d9671\",\"response_data\":[\"Mayor Bill de Blasio has announced that the city plans to fully reopen on July 1st, marking a major milestone in the fight against the COVID-19 pandemic. \\\"We are ready to bring back the life and energy that we all know and love in this city,\\\" said an owner of a small restaurant in Brooklyn. However, public health experts have expressed caution about the city's plans to fully reopen.\"],\"start_time\":\"2023-04-24 05:29:30.218434\",\"status\":\"Success\",\"status_code\":200,\"version_id\":\"7e3864cc-852f-47de-b677-bd3251d46715\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80510fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
