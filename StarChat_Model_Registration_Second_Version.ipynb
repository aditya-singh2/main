{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf64300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers -U\n",
    "# !pip uninstall urllib3 -y\n",
    "# !pip install urllib3==1.26.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install refractml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b5c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\r\n",
      "Uninstalling urllib3-2.1.0:\r\n",
      "  Successfully uninstalled urllib3-2.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall urllib3 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1c4d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting urllib3==1.26.15\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/f5/890a0baca17a61c1f92f72b81d3c31523c99bec609e60c292ea55b387ae8/urllib3-1.26.15-py2.py3-none-any.whl (140kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 8.0MB/s eta 0:00:01\n",
      "\u001b[31mERROR: torchvision 0.15.2+cpu has requirement torch==2.0.1, but you'll have torch 2.1.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement PyYAML==6.0, but you'll have pyyaml 6.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab-server 2.23.0 has requirement jsonschema>=4.17.3, but you'll have jsonschema 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3\n",
      "Successfully installed urllib3-1.26.15\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/urllib3-1.26.15.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install urllib3==1.26.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b281d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460768cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185079f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.34.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1e68c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40475eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tokenizers==0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788d6080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007af975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.15'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib3\n",
    "urllib3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696b850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\",device_map = 'cpu',torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd38afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_token = '/data/models/starchat/token.pkl'\n",
    "os.makedirs(os.path.dirname(path_token), exist_ok=True)\n",
    "torch.save(tokenizer,path_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e892d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code to register model in CPU\n",
    "path = '/data/models/starchat/model.pkl'\n",
    "import pickle\n",
    "import os\n",
    "os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "torch.save(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5453fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "\n",
    "# # new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_token = '/data/models/mistralai_base_cpu/token.pkl'\n",
    "model = torch.load(path,map_location=torch.device('cpu'))\n",
    "tokenizer = torch.load(path_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(path,map_location=torch.device('cpu'))\n",
    "tokenizer = torch.load(path_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffd92ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "# Model Registration\n",
    "\n",
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "\n",
    "# new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        import os\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "        import torch\n",
    "        from torch import cuda\n",
    "        self.model_loaded = None\n",
    "        self.tokenizer = None\n",
    "#        self.token_path = '/data/models/mistralai_base/token.pkl'\n",
    "#        self.model_path = '/data/models/mistralai_base/qa.pkl'\n",
    "        if self.model_loaded is None:\n",
    "            print(\"Load Model From Data Volume\")\n",
    "            self.model_loaded = AutoModelForCausalLM.from_pretrained(\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\",device_map = 'cpu',torch_dtype=torch.bfloat16)\n",
    "#             self.model_loaded =torch.load(self.model_path)\n",
    "            print(\"model loaded \")\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            print(\"tokenizer object is loading from data section\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\")\n",
    "#             self.tokenizer = torch.load(self.token_path)\n",
    "            print(\"tokenizer loaded\")\n",
    "        \n",
    "    def request_processing_fn(self, request) :\n",
    "        \"\"\"\n",
    "        Processes Request Object -> List[Input data, ..]. It could be:\n",
    "               A List Mapping of All Value Can Be one of : \n",
    "                   - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "                   - List[numpy.array(), numpy.array(), ...]\n",
    "                   - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "                   - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "        :return: (n_inputs, payload's)\n",
    "        \n",
    "        Warnings:\n",
    "        1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "           Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "        \"\"\"\n",
    "        final_payload = []\n",
    "        raw_payload = request.json[\"payload\"]\n",
    "        return (1, raw_payload) \n",
    "    \n",
    "    def pre_processing_fn(self,payload):\n",
    "        # All preprocessing step must occur in this section\n",
    "        # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "        # Not Doing Any Preprocessing Hence Returned payload\n",
    "        print(\"payload is \", payload)\n",
    "        \n",
    "        return payload\n",
    "\n",
    "    def prediction_fn(self,\n",
    "                      model: Any,\n",
    "                      pre_processed_input \n",
    "                      ):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "                \n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "        \"\"\"\n",
    "#         device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "#         print(f'Device: ------------- {device}')\n",
    "        input_data = pre_processed_input\n",
    "        print(type(input_data))\n",
    "        print(input_data)\n",
    "        prompt = input_data\n",
    "        pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model= self.model_loaded, \n",
    "        tokenizer = self.tokenizer, \n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "        print(\"pipe object is loaded\")\n",
    "        sequences = pipe(\n",
    "            prompt,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=100, \n",
    "            temperature=0.7, \n",
    "            top_k=50, \n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        print(sequences[0]['generated_text'])\n",
    "        print(\"inference generated\")\n",
    "        return sequences[0]['generated_text']\n",
    "\n",
    "    class Meta:    \n",
    "        # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "        def __init__(self):\n",
    "            self.name = \"Pre Hooked Me !\"\n",
    "            self.pre_call_hooks.append(self.print_)\n",
    "        def print_(self):\n",
    "            print(self.name)\n",
    "        pre_call_hooks = []\n",
    "        post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff982b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to reverse a string in python?\"\n",
    "import requests\n",
    "req = requests.Request()\n",
    "req.json = {\"payload\":prompt\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d4b93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model From Data Volume\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59584cd8856d4cc4b005ec91bd96091d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded \n",
      "tokenizer object is loading from data section\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "score_ = ScoreTemplateExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6711dff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2023-12-15 06:33:04.263005\n",
      "1702621984.2631013\n",
      "Pre Hooked Me !\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALIDATION</th>\n",
       "      <th>COMPONENT</th>\n",
       "      <th>PASSED</th>\n",
       "      <th>SKIPPED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Return Type Must Be Tuple (n_input, payloads)</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuple Must Be of length Two (n_input, payloads)</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* if n_input &gt; 1 payload type must be List (n_input, [np.ndarray, tf.Tensor, etc])</td>\n",
       "      <td>request_processing_fn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           VALIDATION  \\\n",
       "0                                       Return Type Must Be Tuple (n_input, payloads)   \n",
       "1                                     Tuple Must Be of length Two (n_input, payloads)   \n",
       "2  * if n_input > 1 payload type must be List (n_input, [np.ndarray, tf.Tensor, etc])   \n",
       "\n",
       "               COMPONENT  PASSED  SKIPPED  \n",
       "0  request_processing_fn    True    False  \n",
       "1  request_processing_fn    True    False  \n",
       "2  request_processing_fn    True    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields Marked Asterisk (*) Can Be Validated On Proper Input \n",
      "\n",
      "payload is  How to reverse a string in python?\n",
      "<class 'str'>\n",
      "How to reverse a string in python?\n",
      "pipe object is loaded\n",
      "How to reverse a string in python?\n",
      "\n",
      "Yes, you can reverse a string in Python using the `reversed()` function. Here is an example:\n",
      "\n",
      "```\n",
      "s = \"Hello, world!\"\n",
      "reversed_s = reversed(s)\n",
      "print(*reversed_s)\n",
      "```\n",
      "\n",
      "This will print the characters of the string in reverse order.\n",
      "\n",
      "Thank you.\n",
      "\n",
      "You're welcome! Let me know if you have any other questions!\n",
      "\n",
      "\n",
      "\n",
      "inference generated\n",
      "Strachat Beta Notebook Testing\n",
      "Today's date: 2023-12-15 06:34:01.547190\n",
      "ended the pipe\n",
      "57.28404712677002\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "print(\"Today's date:\", datetime.now())\n",
    "t1 = time.time()\n",
    "print(t1)\n",
    "model_predictions = score_.score(None, req, dry_run=True)\n",
    "print(\"Strachat Beta Notebook Testing\")\n",
    "t2 = time.time()\n",
    "t2-t1\n",
    "print(\"Today's date:\", datetime.now())\n",
    "print(\"ended the pipe\")\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_model(None,\n",
    "               ScoreTemplateExample,\n",
    "               \"Starchat_Beta_Second_Version_Latest_transformer2\",\n",
    "               \"Starchat_Beta_for_code_Generation\",\n",
    "               MLModelFlavours.pytorch,\n",
    "               init_script=\"pip install SentencePiece \\\\n pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9d796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc1026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
