{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1ce2c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/packages/b1bcda690655777373f57ea6614eb095ec2c886f does not appear to have a file named config.json. Checkout 'https://huggingface.co//packages/b1bcda690655777373f57ea6614eb095ec2c886f/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/packages/b1bcda690655777373f57ea6614eb095ec2c886f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/pipelines/__init__.py:705\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 705\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    708\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:983\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    982\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 983\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    985\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:617\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    619\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:388\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    389\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /packages/b1bcda690655777373f57ea6614eb095ec2c886f does not appear to have a file named config.json. Checkout 'https://huggingface.co//packages/b1bcda690655777373f57ea6614eb095ec2c886f/None' for available files."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"show me the name of flowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "print(\"before model start\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "print(\"aavesh\")\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81f5cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outputs[0][\"generated_text\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a957e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.34.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5453fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "\n",
    "# # new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51831cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_loaded = None\n",
    "\n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "\n",
    "        if self.model_loaded is None:\n",
    "            print(\"LLM model loading from data section\")\n",
    "            self.model_loaded = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "    def request_processing_fn(self, request) :\n",
    "    \"\"\"\n",
    "    Processes Request Object -> List[Input data, ..]. It could be:\n",
    "           A List Mapping of All Value Can Be one of : \n",
    "               - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "               - List[numpy.array(), numpy.array(), ...]\n",
    "               - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "               - List[ SingleSample, SingleSample]\n",
    "\n",
    "    :return: (n_inputs, payload's)\n",
    "\n",
    "    Warnings:\n",
    "    1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "       Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "    \"\"\"\n",
    "    final_payload = []\n",
    "    raw_payload = request.json[\"payload\"]\n",
    "    return (1, raw_payload) \n",
    "    \n",
    "     def pre_processing_fn(self,payload):\n",
    "        \"\"\"\n",
    "         \"\"\"\n",
    "        return payload\n",
    "\n",
    "     def prediction_fn(self,\n",
    "                       model: Any,\n",
    "                       input_query \n",
    "                       ):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "                \n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "        \"\"\"\n",
    "        prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "        text = input_query\n",
    "        prompt = prompt_template.format(query=text)\n",
    "#         # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "        print(\"promting has started\")\n",
    "        outputs = self.model_loaded(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "        preds = str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "        print(\"prediction is \\n: \",preds)\n",
    "          return preds\n",
    "        \n",
    "#     class Meta:    \n",
    "         # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "#         def __init__(self):\n",
    "#             self.name = \"Pre Hooked Me !\"\n",
    "#             self.pre_call_hooks.append(self.print_)\n",
    "#         def print_(self):\n",
    "#             print(self.name)\n",
    "#             pre_call_hooks = []\n",
    "#             post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc341beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ = ScoreTemplateExample()\n",
    "import requests\n",
    "req = requests.Request()\n",
    "\n",
    "req.json = {\"payload\":\"How do I sort a list in Python?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ = ScoreTemplateExample()\n",
    "import requests\n",
    "req = requests.Request()\n",
    "\n",
    "req.json = {\"payload\":\"How do I build neural network in Python?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = time.time()\n",
    "model_predictions = score_.score(None, req, dry_run=True)\n",
    "t4 = time.time()\n",
    "t4-t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364fcb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_model(model,\n",
    "               ScoreTemplateExample,\n",
    "               \"Starchat_Beta\",\n",
    "               \"Starchat_Beta_for_code_Generation\",\n",
    "               MLModelFlavours.pytorch,\n",
    "               init_script=\"pip install SentencePiece \\\\n pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9d796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
