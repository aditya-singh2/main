{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1ce2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e584d528b2140c692a6cc6765fb7619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc17593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "627.7025089263916"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "print(\"before model start\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb81f5cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|system|>',\n",
       " '<|end|>',\n",
       " '<|user|>',\n",
       " 'How do I sort a list in Python?<|end|>',\n",
       " '<|assistant|>',\n",
       " 'There are multiple ways to sort a list in Python. One of the most common ways is using the sort() method.',\n",
       " '',\n",
       " \"Here's an example:\",\n",
       " '',\n",
       " '```',\n",
       " 'numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]',\n",
       " 'numbers.sort()',\n",
       " 'print(numbers)',\n",
       " '```',\n",
       " '',\n",
       " 'This will sort the list in place and print the sorted list.',\n",
       " '',\n",
       " 'Another way to sort a list is using the sorted() function. This function returns a new sorted list without modifying the original list.',\n",
       " '',\n",
       " \"Here's an example:\",\n",
       " '',\n",
       " '```',\n",
       " 'numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]',\n",
       " 'sorted_numbers = sorted(numbers)',\n",
       " 'print(sorted_numbers)',\n",
       " '```',\n",
       " '',\n",
       " 'In this example, the sorted() function is used to create a new sorted list, and the original list is not modified.',\n",
       " '',\n",
       " 'There are also other sorting algorithms available in the built-in sort module, such as quicksort, heapsort, and merge sort. You can learn more about these algorithms and how to']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][\"generated_text\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a957e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.34.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5453fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "\n",
    "# # new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a304df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreTemplateExample(ScoreBase):\n",
    "#     \"\"\"\n",
    "#     This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "#     \"\"\"    \n",
    "     def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_loaded = None\n",
    "        \n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "\n",
    "        if self.model_loaded is None:\n",
    "            print(\"LLM model loading from data section\")\n",
    "            self.model_loaded = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "     def request_processing_fn(self, request) :\n",
    "#         \"\"\"\n",
    "#         Processes Request Object -> List[Input data, ..]. It could be:\n",
    "#                A List Mapping of All Value Can Be one of : \n",
    "#                    - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "#                    - List[numpy.array(), numpy.array(), ...]\n",
    "#                    - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "#                    - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "#         :return: (n_inputs, payload's)\n",
    "        \n",
    "#         Warnings:\n",
    "#         1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "#            Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "#         \"\"\"\n",
    "         final_payload = []\n",
    "         raw_payload = request.json[\"payload\"]\n",
    "         return (1, raw_payload) \n",
    "    \n",
    "     def pre_processing_fn(self,payload):\n",
    "#         # All preprocessing step must occur in this section\n",
    "#         # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "#         # Not Doing Any Preprocessing Hence Returned payload\n",
    "#         print(\"payload is \", payload)\n",
    "        \n",
    "         return payload\n",
    "\n",
    "     def prediction_fn(self,\n",
    "                       model: Any,\n",
    "                       input_query \n",
    "                       ):\n",
    "#         \"\"\"\n",
    "#                 Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "#                 :param model: Supported Model\n",
    "#                 :param pre_processed_input: Single Preprocessed Payload\n",
    "#                 :return: Prediction Value From the model\n",
    "                \n",
    "#                 Important Notes:\n",
    "#                 - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "#         \"\"\"\n",
    "#         model_loaded = self.model_loaded\n",
    "#         mod = model_loaded\n",
    "#         text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "        \n",
    "          prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "          text = input_query\n",
    "          prompt = prompt_template.format(query=text)\n",
    "#         # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "          outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "          preds = str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "        \n",
    "        \n",
    "          print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "          return preds\n",
    "\n",
    "     class Meta:    \n",
    "         # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "         def __init__(self):\n",
    "             self.name = \"Pre Hooked Me !\"\n",
    "             self.pre_call_hooks.append(self.print_)\n",
    "         def print_(self):\n",
    "             print(self.name)\n",
    "             pre_call_hooks = []\n",
    "             post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db92a342",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Meta' object has no attribute 'pre_call_hooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m score_ \u001b[38;5;241m=\u001b[39m \u001b[43mScoreTemplateExample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m req \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mRequest()\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mScoreTemplateExample.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m    \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m    \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/mosaic_utils/ai/score/base.py:35\u001b[0m, in \u001b[0;36mScoreBase.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_response_collection \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[12], line 80\u001b[0m, in \u001b[0;36mScoreTemplateExample.Meta.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre Hooked Me !\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_call_hooks\u001b[49m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Meta' object has no attribute 'pre_call_hooks'"
     ]
    }
   ],
   "source": [
    "score_ = ScoreTemplateExample()\n",
    "import requests\n",
    "req = requests.Request()\n",
    "\n",
    "req.json = {\"payload\":\"How do I sort a list in Python?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f1499b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mscore_\u001b[49m\u001b[38;5;241m.\u001b[39mscore(model, req, dry_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_' is not defined"
     ]
    }
   ],
   "source": [
    "model_predictions = score_.score(model, req, dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138a295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
