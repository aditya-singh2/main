{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82315d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25d1848ce354c3eba4ce8e9bcad310b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13da54bbc85544db96bf3cecc5c85821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00004.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cfbbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130.81290984153748"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb81f5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|system|>',\n",
       " '<|end|>',\n",
       " '<|user|>',\n",
       " 'How do I sort a list in Python?<|end|>',\n",
       " '<|assistant|>',\n",
       " 'There are several ways to sort a list in Python. The most common way is to use the sort() method.',\n",
       " '',\n",
       " 'Here is an example:',\n",
       " '',\n",
       " '```',\n",
       " 'numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]',\n",
       " 'numbers.sort()',\n",
       " 'print(numbers)',\n",
       " 'shuffle(numbers)',\n",
       " '```',\n",
       " '',\n",
       " 'This will sort the list in place and print the sorted list.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][\"generated_text\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4432b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.593522787094116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=10, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1767bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting gradio\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8f0c3b640>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/05/f2/360ca9546cffa45fee1df56864fdc2b6955de622e98435539490cd882a96/gradio-3.47.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/f2/360ca9546cffa45fee1df56864fdc2b6955de622e98435539490cd882a96/gradio-3.47.1-py3-none-any.whl (20.3MB)\n",
      "\u001b[K     |████████████████████████████████| 20.3MB 7.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml<7.0,>=5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/6b/6600ac24725c7388255b2f5add93f91e58a5d7efaf4af244fdbcc11a541b/PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736kB)\n",
      "\u001b[K     |████████████████████████████████| 737kB 78.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gradio-client==0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/dd/db8dcc8521aa475a18a562929de0806819f8fa73ee8654d2cc22c836c3bd/gradio_client-0.6.0-py3-none-any.whl (298kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 108.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests~=2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 101.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting altair<6.0,>=4.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/16/b12fca347ff9d062e3c44ad9641d2ec50364570a059f3078ada3a5119d7a/altair-5.1.2-py3-none-any.whl (516kB)\n",
      "\u001b[K     |████████████████████████████████| 522kB 83.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow<11.0,>=8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/ae/4050c0d3ca5d3106cbbce376c58c061dce0a6ff77dae6a30f8e3d285ee7c/Pillow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5MB 125.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting uvicorn>=0.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 101.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib~=3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/73/a4af3493a81d6e5e1fdb4c72f4d3573a7e94b60f7c2c69ab0275fdc7cd8e/matplotlib-3.7.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2MB 109.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiofiles<24.0,>=22.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl\n",
      "Collecting packaging\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 92.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas<3.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/7f/5b047effafbdd34e52c9e2d7e44f729a0655efafb22198c45cf692cdc157/pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4MB 106.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydub\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl\n",
      "Collecting python-multipart\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/ff/b1e11d8bffb5e0e1b6d27f402eeedbeb9be6df2cdbc09356a1ae49806dbf/python_multipart-0.0.6-py3-none-any.whl (45kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 87.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2<4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/c3/f068337a370801f372f2f8f6bad74a5c140f6fda3d9de154052708dd3c65/Jinja2-3.1.2-py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 121.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastapi\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/d2/3ad038a2365fefbac19d9a046cab7ce45f4c7bfa81d877cbece9707de9ce/fastapi-0.103.2-py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 98.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/66/0a72c9fcde42e5650c8d8d5c5c1873b9a3893018020c77ca8eb62708b923/pydantic-2.4.2-py3-none-any.whl (395kB)\n",
      "\u001b[K     |████████████████████████████████| 399kB 130.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 111.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 104.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting semantic-version~=2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/23/8146aad7d88f4fcb3a6218f41a60f6c2d4e3a72de72da1825dc7c8f7877c/semantic_version-2.10.0-py2.py3-none-any.whl\n",
      "Collecting typing-extensions~=4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl\n",
      "Collecting markupsafe~=2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/de/e2/32c14301bb023986dff527a49325b6259cab4ebb4633f69de54af312fc45/MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting importlib-resources<7.0,>=1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/65/6e/09d8816b5cb7a4006ef8ad1717a2703ad9f331dae9717d9f22488a2d6469/importlib_resources-6.1.0-py3-none-any.whl\n",
      "Collecting websockets<12.0,>=10.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/05/2efb520317340ece74bfc4d88e8f011dd71a4e6c263000bfffb71a343685/websockets-11.0.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 111.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5d/5738903efe0ecb73e51eb44feafba32bdba2081263d40c5043568ff60faf/numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 105.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/06/49b275a312eb207e2a2718a7414dedfded05088437352b67aaa9a355f948/ffmpy-0.3.1.tar.gz\n",
      "Collecting orjson~=3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/45/c9168772b1d1360525287df9e7d3e807f636841f681787e25f7cf3b36a5c/orjson-3.9.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 94.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl (173kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 115.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 104.1MB/s eta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/40/9957270221b6d3e9a3b92fdfba80dd5c9661ff45a664b47edd5d00f707f5/urllib3-2.0.6-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 105.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/34/3030de6f1370931b9dbb4dad48f6ab1015ab1d32447850b9fc94e60097be/idna-3.4-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 108.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c8/fd52271326c052f95f47ef718b018aa2bc3fd097d9bac44d7d48894c6130/charset_normalizer-3.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 126.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toolz\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/5c/922a3508f5bda2892be3df86c74f9cf1e01217c2b1f8a0ac4841d903e3e9/toolz-0.12.0-py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 94.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema>=3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/bf/a84bc75f069f4f156e1c0d9892fb7325945106c6ecaad9f29d24360872af/jsonschema-4.19.1-py3-none-any.whl (83kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 103.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click>=7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 106.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting h11>=0.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 93.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/71/7f20855592cc929bc206810432b991ec4c702dc26b0567b132e52c85536f/contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 98.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/55/7021ffcc8cb26a520bb051aa0a3d08daf200cde945e5863d5768161e2d3d/kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 111.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl (247kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 108.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 116.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl\n",
      "Collecting fonttools>=4.22.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b8/965acdc0510d0ceca51bb9b9286a600eb8221d1e79d29da16f5bf035deab/fonttools-4.43.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6MB 99.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl (502kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 84.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/fb/a79efcab32b8a1f1ddca7f35109a50e4a80d42ac1c9187ab46522b2407d7/tzdata-2023.3-py2.py3-none-any.whl (341kB)\n",
      "\u001b[K     |████████████████████████████████| 348kB 110.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 74.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting anyio<4.0.0,>=3.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 98.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic-core==2.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/82/4139ea7aebec9e22be42d8a5342df5022b7b6b6deb10126dc3ff80b4d6fd/pydantic_core-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 97.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl\n",
      "Collecting tqdm>=4.42.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 87.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl\n",
      "Collecting sniffio\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/a0/5dba8ed157b0136607c7f2151db695885606968d1fae123dc3391e0cfdbf/sniffio-1.3.0-py3-none-any.whl\n",
      "Collecting httpcore<0.19.0,>=0.18.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 99.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=3.1.0; python_version < \"3.10\"\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/24/83349ac2189cc2435e84da3f69ba3c97314d3c0622628e55171c6798ed80/jsonschema_specifications-2023.7.1-py3-none-any.whl\n",
      "Collecting attrs>=22.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/eb/fcb708c7bf5056045e9e98f62b93bd7467eb718b0202e7698eb11d66416c/attrs-23.1.0-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 87.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rpds-py>=0.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/a1/3c46b6593e54982779e1d96b53ad4db275d8ca4bb43dae37d547e56f64e0/rpds_py-0.10.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 101.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting referencing>=0.28.4\n",
      "  Downloading https://files.pythonhosted.org/packages/be/8e/56d6f1e2d591f4d6cbcba446cac4a1b0dc4f584537e2071d9bcee8eeab6b/referencing-0.30.2-py3-none-any.whl\n",
      "Collecting pkgutil-resolve-name>=1.3.10; python_version < \"3.9\"\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/5c/3d4882ba113fd55bdba9326c1e4c62a15e674a2501de4869e6bd6301f87e/pkgutil_resolve_name-1.3.10-py3-none-any.whl\n",
      "Collecting six>=1.5\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting exceptiongroup; python_version < \"3.11\"\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/83/b71e58666f156a39fb29417e4c8ca4bc7400c0dd4ed9e8842ab54dc8c344/exceptiongroup-1.1.3-py3-none-any.whl\n",
      "Building wheels for collected packages: ffmpy\n",
      "\u001b[33m  WARNING: Building wheel for ffmpy failed: [Errno 13] Permission denied: '/home/mosaic-ai/.cache/pip/wheels/f2'\u001b[0m\n",
      "Failed to build ffmpy\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement PyYAML==6.0, but you'll have pyyaml 6.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement urllib3==1.26.15, but you'll have urllib3 2.0.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.0.0a1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-auth 2.22.0 has requirement urllib3<2.0, but you'll have urllib3 2.0.6 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyyaml, typing-extensions, fsspec, websockets, packaging, certifi, urllib3, idna, charset-normalizer, requests, tqdm, filelock, huggingface-hub, sniffio, h11, exceptiongroup, anyio, httpcore, httpx, gradio-client, toolz, zipp, importlib-resources, rpds-py, attrs, referencing, jsonschema-specifications, pkgutil-resolve-name, jsonschema, numpy, pytz, six, python-dateutil, tzdata, pandas, markupsafe, jinja2, altair, pillow, click, uvicorn, contourpy, kiwisolver, pyparsing, cycler, fonttools, matplotlib, aiofiles, pydub, python-multipart, starlette, pydantic-core, annotated-types, pydantic, fastapi, semantic-version, ffmpy, orjson, gradio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Running setup.py install for ffmpy ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed aiofiles-23.2.1 altair-5.1.2 annotated-types-0.6.0 anyio-3.7.1 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.0 click-8.1.7 contourpy-1.1.1 cycler-0.12.1 exceptiongroup-1.1.3 fastapi-0.103.2 ffmpy-0.3.1 filelock-3.12.4 fonttools-4.43.1 fsspec-2023.9.2 gradio-3.47.1 gradio-client-0.6.0 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 idna-3.4 importlib-resources-6.1.0 jinja2-3.1.2 jsonschema-4.19.1 jsonschema-specifications-2023.7.1 kiwisolver-1.4.5 markupsafe-2.1.3 matplotlib-3.7.3 numpy-1.24.4 orjson-3.9.9 packaging-23.2 pandas-2.0.3 pillow-10.0.1 pkgutil-resolve-name-1.3.10 pydantic-2.4.2 pydantic-core-2.10.1 pydub-0.25.1 pyparsing-3.1.1 python-dateutil-2.8.2 python-multipart-0.0.6 pytz-2023.3.post1 pyyaml-6.0.1 referencing-0.30.2 requests-2.31.0 rpds-py-0.10.6 semantic-version-2.10.0 six-1.16.0 sniffio-1.3.0 starlette-0.27.0 toolz-0.12.0 tqdm-4.66.1 typing-extensions-4.8.0 tzdata-2023.3 urllib3-2.0.6 uvicorn-0.23.2 websockets-11.0.3 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f434badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-y67o66nx because the default path (/home/mosaic-ai/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a7b8e7f84940bcea4f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a7b8e7f84940bcea4f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "examples = [\n",
    "    \"How can I write a Python function to generate the nth Fibonacci number?\",\n",
    "    \"How do I get the current date using shell commands? Explain how it works.\",\n",
    "    \"What's the meaning of life?\",\n",
    "    \"Write a function in Javascript to reverse words in a given string.\",\n",
    "    \"Give the following data {'Name':['Tom', 'Brad', 'Kyle', 'Jerry'], 'Age':[20, 21, 19, 18], 'Height' : [6.1, 5.9, 6.0, 6.1]}. Can you plot one graph with two subplots as columns. The first is a bar graph showing the height of each person. The second is a bargraph showing the age of each person? Draw the graph in seaborn talk mode.\",\n",
    "    \"Create a regex to extract dates from logs\",\n",
    "    \"How to decode JSON into a typescript object\",\n",
    "    \"Write a list into a jsonlines file and save locally\",\n",
    "]\n",
    "\n",
    "def echo(message, history):\n",
    "    \n",
    "    prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "    prompt = prompt_template.format(query=message)\n",
    "    # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "    outputs = pipe(prompt, max_new_tokens=356, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "    return str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "    \n",
    "demo = gr.ChatInterface(fn=echo, examples=examples, title=\"StarChat Bot\")\n",
    "#demo.launch(share=True)\n",
    "demo.queue(concurrency_count=3).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d067b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/refs/main\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f86073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--CodeFuse-13B\r\n",
      "models--HuggingFaceH4--starchat-beta\r\n",
      "models--Salesforce--codegen-350M-mono\r\n",
      "models--TheBloke--Redmond-Hermes-Coder-GGML\r\n",
      "models--TheBloke--Redmond-Hermes-Coder-GPTQ\r\n",
      "models--TheBloke--starcoderplus-GPTQ\r\n",
      "models--bigcode--starcoderbase-1b\r\n",
      "models--bigcode--starcoderplus\r\n",
      "models--bigcode--tiny_starcoder_py\r\n",
      "models--microsoft--phi-1_5\r\n",
      "models--mistralai--Mistral-7B-Instruct-v0.1\r\n",
      "tmp9zao9gd7\r\n",
      "tmpjox9s4fp\r\n",
      "version.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/huggingface/cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1ce2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5f1ce9b1954097b3b3b0a7b89a0646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c1c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c8fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=10, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc17593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120.0493311882019"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "print(\"before model start\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ec8e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/notebooks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c56792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "\n",
    "# new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5934ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_loaded = None\n",
    "        \n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "\n",
    "        if self.model_loaded is None:\n",
    "            print(\"LLM model loading from data section\")\n",
    "            self.model_loaded = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "    def request_processing_fn(self, request) :\n",
    "        \"\"\"\n",
    "        Processes Request Object -> List[Input data, ..]. It could be:\n",
    "               A List Mapping of All Value Can Be one of : \n",
    "                   - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "                   - List[numpy.array(), numpy.array(), ...]\n",
    "                   - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "                   - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "        :return: (n_inputs, payload's)\n",
    "        \n",
    "        Warnings:\n",
    "        1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "           Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "        \"\"\"\n",
    "        final_payload = []\n",
    "        raw_payload = request.json[\"payload\"]\n",
    "        return (1, raw_payload) \n",
    "    \n",
    "    def pre_processing_fn(self,payload):\n",
    "        # All preprocessing step must occur in this section\n",
    "        # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "        # Not Doing Any Preprocessing Hence Returned payload\n",
    "        print(\"payload is \", payload)\n",
    "        \n",
    "        return payload\n",
    "\n",
    "    def prediction_fn(self,\n",
    "                      model: Any,\n",
    "                      pre_processed_input \n",
    "                      ):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "                \n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "        \"\"\"\n",
    "        model_loaded = self.model_loaded\n",
    "        mod = model_loaded\n",
    "        text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "        \n",
    "        prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "        prompt = prompt_template.format(query=text)\n",
    "        # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "        outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "        preds = str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "        \n",
    "        \n",
    "        print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "        return preds\n",
    "\n",
    "    class Meta:    \n",
    "        # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "        def __init__(self):\n",
    "            self.name = \"Pre Hooked Me !\"\n",
    "            self.pre_call_hooks.append(self.print_)\n",
    "        def print_(self):\n",
    "            print(self.name)\n",
    "        pre_call_hooks = []\n",
    "        post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6301f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        import pickle\n",
    "        from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "        self.model_loaded = None\n",
    "        self.tokenizer = None\n",
    "        if self.model_loaded is None:\n",
    "            print(\"LLM model loading from data section\")\n",
    "            self.model_loaded = T5ForConditionalGeneration.from_pretrained(\"/data/artifacts_gpu/model/\")\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            print(\"tokenizer object is loading from data section\")\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(\"/data/artifacts_gpu/tokenizer/\")\n",
    "        \n",
    "    \n",
    "    def request_processing_fn(self, request) :\n",
    "        \"\"\"\n",
    "        Processes Request Object -> List[Input data, ..]. It could be:\n",
    "               A List Mapping of All Value Can Be one of : \n",
    "                   - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "                   - List[numpy.array(), numpy.array(), ...]\n",
    "                   - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "                   - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "        :return: (n_inputs, payload's)\n",
    "        \n",
    "        Warnings:\n",
    "        1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "           Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "        \"\"\"\n",
    "        final_payload = []\n",
    "        raw_payload = request.json[\"payload\"]\n",
    "        return (1, raw_payload) \n",
    "    \n",
    "    def pre_processing_fn(self,payload):\n",
    "        # All preprocessing step must occur in this section\n",
    "        # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "        # Not Doing Any Preprocessing Hence Returned payload\n",
    "        print(\"payload is \", payload)\n",
    "        \n",
    "        return payload\n",
    "\n",
    "    def prediction_fn(self,\n",
    "                      model: Any,\n",
    "                      pre_processed_input \n",
    "                      ):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "                \n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "        \"\"\"\n",
    "        model_loaded = self.model_loaded\n",
    "        mod = model_loaded\n",
    "        text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "        tokenizer = self.tokenizer\n",
    "        source = tokenizer.batch_encode_plus([text], max_length=512, pad_to_max_length=True,return_tensors='pt')\n",
    "        source_ids = source['input_ids']\n",
    "        source_mask = source['attention_mask']\n",
    "        generated_ids = mod.generate(\n",
    "            input_ids = source_ids,\n",
    "            attention_mask = source_mask, \n",
    "            max_length=150, \n",
    "            num_beams=2,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True\n",
    "\n",
    "        )\n",
    "        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "        print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "        return preds\n",
    "\n",
    "    class Meta:    \n",
    "        # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "        def __init__(self):\n",
    "            self.name = \"Pre Hooked Me !\"\n",
    "            self.pre_call_hooks.append(self.print_)\n",
    "        def print_(self):\n",
    "            print(self.name)\n",
    "        pre_call_hooks = []\n",
    "        post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34804572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: streamlit: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!streamlit run /opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb83123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10sec.py\r\n",
      " Dashboard_App2\r\n",
      " Falcon-7B.ipynb\r\n",
      " Flant5-xl-cpu.ipynb\r\n",
      " Flant5-xl-gpu.ipynb\r\n",
      "'GPU Utilisation Check.ipynb'\r\n",
      " Live_Usage_queries_with_mql_processed_rev3_curated.csv\r\n",
      " LlamaCoder.ipynb\r\n",
      " Microsoft-phi1.5.ipynb\r\n",
      " Octocoder.ipynb\r\n",
      " README.md\r\n",
      "'SCB StarChat Demo.ipynb'\r\n",
      "'SCB coder-GPU.ipynb'\r\n",
      "'SCB coder.ipynb'\r\n",
      " Startcoder-tiny.ipynb\r\n",
      " T5_sum_cpu.ipynb\r\n",
      " T5_sum_gpu.ipynb\r\n",
      " Untitled.ipynb\r\n",
      " Untitled1.ipynb\r\n",
      " Untitled2.ipynb\r\n",
      " Untitled3.ipynb\r\n",
      " __MACOSX\r\n",
      " __pycache__\r\n",
      " chatAppSCB5\r\n",
      " dash_gunicorn\r\n",
      " dialogues.py\r\n",
      " falcon_7b_summarization.ipynb\r\n",
      " gradio_app\r\n",
      " image_classification.ipynb\r\n",
      " llama-2-7b-fine-tuned-peft-v1\r\n",
      " llama-2-7b-fine-tuned-peft-v2\r\n",
      " llama2-fine-tuning-example.ipynb\r\n",
      "'llama2-fine-tuning-user-query-to-template-peft (1).ipynb'\r\n",
      " llama2-fine-tuning-user-query-to-template-peft.ipynb\r\n",
      " mistral-ft-code-generation-tryout.ipynb\r\n",
      " mistral-ft-user-query-to-template-peft-lora.ipynb\r\n",
      " predict_df1_0_100.csv\r\n",
      " predict_df_100_200.csv\r\n",
      " predict_df_200_300.csv\r\n",
      " predict_df_300_500.csv\r\n",
      " random.ipynb\r\n",
      " share_btn.py\r\n",
      " start_code\r\n",
      " streamlit\r\n",
      " streamlit_app\r\n",
      " streamlit_app_new\r\n",
      " template_and_user_query.csv\r\n",
      " template_and_user_query_v2.csv\r\n",
      " test_trainer\r\n",
      " thumbnail.png\r\n",
      " trainer_tf.ipynb\r\n",
      " trainer_torch.ipynb\r\n",
      " val_df.csv\r\n",
      " val_df_prediction.csv\r\n",
      " vit-base-beans\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8345054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nadded_tokens.json\\t\\t  \\nmodel-00004-of-00004.safetensors\\nconfig.json\\t\\t\\t  \\nmodel.safetensors.index.json\\ngeneration_config.json\\t\\t  \\nspecial_tokens_map.json\\nmerges.txt\\t\\t\\t  \\ntokenizer.json\\nmodel-00001-of-00004.safetensors  \\ntokenizer_config.json\\nmodel-00002-of-00004.safetensors  \\nvocab.json\\nmodel-00003-of-00004.safetensors\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "added_tokens.json\t\t  \n",
    "model-00004-of-00004.safetensors\n",
    "config.json\t\t\t  \n",
    "model.safetensors.index.json\n",
    "generation_config.json\t\t  \n",
    "special_tokens_map.json\n",
    "merges.txt\t\t\t  \n",
    "tokenizer.json\n",
    "model-00001-of-00004.safetensors  \n",
    "tokenizer_config.json\n",
    "model-00002-of-00004.safetensors  \n",
    "vocab.json\n",
    "model-00003-of-00004.safetensors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc4e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31dcf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d4b791697645b3b65e096216a66746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e828fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pipeline in module transformers.pipelines:\n",
      "\n",
      "pipeline(task: str = None, model: Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, image_processor: Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None, framework: Union[str, NoneType] = None, revision: Union[str, NoneType] = None, use_fast: bool = True, use_auth_token: Union[str, bool, NoneType] = None, device: Union[int, str, ForwardRef('torch.device'), NoneType] = None, device_map=None, torch_dtype=None, trust_remote_code: Union[bool, NoneType] = None, model_kwargs: Dict[str, Any] = None, pipeline_class: Union[Any, NoneType] = None, **kwargs) -> transformers.pipelines.base.Pipeline\n",
      "    Utility factory method to build a [`Pipeline`].\n",
      "    \n",
      "    Pipelines are made of:\n",
      "    \n",
      "        - A [tokenizer](tokenizer) in charge of mapping raw textual input to token.\n",
      "        - A [model](model) to make predictions from the inputs.\n",
      "        - Some (optional) post processing for enhancing model's output.\n",
      "    \n",
      "    Args:\n",
      "        task (`str`):\n",
      "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
      "    \n",
      "            - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
      "            - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
      "            - `\"conversational\"`: will return a [`ConversationalPipeline`].\n",
      "            - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
      "            - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
      "            - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
      "            - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
      "            - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
      "            - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
      "            - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
      "            - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
      "            - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
      "            - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
      "            - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
      "            - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
      "            - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
      "            - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
      "              [`TextClassificationPipeline`].\n",
      "            - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
      "            - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
      "            - `\"translation\"`: will return a [`TranslationPipeline`].\n",
      "            - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
      "            - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
      "            - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
      "            - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
      "            - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
      "            - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
      "            - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`].\n",
      "    \n",
      "        model (`str` or [`PreTrainedModel`] or [`TFPreTrainedModel`], *optional*):\n",
      "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
      "            actual instance of a pretrained model inheriting from [`PreTrainedModel`] (for PyTorch) or\n",
      "            [`TFPreTrainedModel`] (for TensorFlow).\n",
      "    \n",
      "            If not provided, the default for the `task` will be loaded.\n",
      "        config (`str` or [`PretrainedConfig`], *optional*):\n",
      "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
      "            identifier or an actual pretrained model configuration inheriting from [`PretrainedConfig`].\n",
      "    \n",
      "            If not provided, the default configuration file for the requested model will be used. That means that if\n",
      "            `model` is given, its default configuration will be used. However, if `model` is not supplied, this\n",
      "            `task`'s default model's config is used instead.\n",
      "        tokenizer (`str` or [`PreTrainedTokenizer`], *optional*):\n",
      "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained tokenizer inheriting from [`PreTrainedTokenizer`].\n",
      "    \n",
      "            If not provided, the default tokenizer for the given `model` will be loaded (if it is a string). If `model`\n",
      "            is not specified or not a string, then the default tokenizer for `config` is loaded (if it is a string).\n",
      "            However, if `config` is also not given or not a string, then the default tokenizer for the given `task`\n",
      "            will be loaded.\n",
      "        feature_extractor (`str` or [`PreTrainedFeatureExtractor`], *optional*):\n",
      "            The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained feature extractor inheriting from [`PreTrainedFeatureExtractor`].\n",
      "    \n",
      "            Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
      "            models. Multi-modal models will also require a tokenizer to be passed.\n",
      "    \n",
      "            If not provided, the default feature extractor for the given `model` will be loaded (if it is a string). If\n",
      "            `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n",
      "            is a string). However, if `config` is also not given or not a string, then the default feature extractor\n",
      "            for the given `task` will be loaded.\n",
      "        framework (`str`, *optional*):\n",
      "            The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "            installed.\n",
      "    \n",
      "            If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "            provided.\n",
      "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "            When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
      "            branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
      "            artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n",
      "        use_fast (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n",
      "        use_auth_token (`str` or *bool*, *optional*):\n",
      "            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      "            when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      "        device (`int` or `str` or `torch.device`):\n",
      "            Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n",
      "            pipeline will be allocated.\n",
      "        device_map (`str` or `Dict[str, Union[int, str, torch.device]`, *optional*):\n",
      "            Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate` library is present, set\n",
      "            `device_map=\"auto\"` to compute the most optimized `device_map` automatically (see\n",
      "            [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)\n",
      "            for more information).\n",
      "    \n",
      "            <Tip warning={true}>\n",
      "    \n",
      "            Do not use `device_map` AND `device` at the same time as they will conflict\n",
      "    \n",
      "            </Tip>\n",
      "    \n",
      "        torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      "            Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      "            (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`).\n",
      "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to allow for custom code defined on the Hub in their own modeling, configuration,\n",
      "            tokenization or even pipeline files. This option should only be set to `True` for repositories you trust\n",
      "            and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
      "        model_kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\n",
      "            **model_kwargs)` function.\n",
      "        kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
      "            corresponding pipeline class for possible values).\n",
      "    \n",
      "    Returns:\n",
      "        [`Pipeline`]: A suitable pipeline for the task.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    ```python\n",
      "    >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
      "    \n",
      "    >>> # Sentiment analysis pipeline\n",
      "    >>> analyzer = pipeline(\"sentiment-analysis\")\n",
      "    \n",
      "    >>> # Question answering pipeline, specifying the checkpoint identifier\n",
      "    >>> oracle = pipeline(\n",
      "    ...     \"question-answering\", model=\"distilbert-base-cased-distilled-squad\", tokenizer=\"bert-base-cased\"\n",
      "    ... )\n",
      "    \n",
      "    >>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
      "    >>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      "    >>> recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df528d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.10747027397156"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709e074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
