{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82315d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cfbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We use a variant of ChatML to format each message\n",
    "# import time\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "# prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "# outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "# t2 = time.time()\n",
    "# t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb81f5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|system|>',\n",
       " '<|end|>',\n",
       " '<|user|>',\n",
       " 'How do I sort a list in Python?<|end|>',\n",
       " '<|assistant|>',\n",
       " 'There are multiple ways to sort a list in']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][\"generated_text\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4432b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We use a variant of ChatML to format each message\n",
    "# import time\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "# prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "# outputs = pipe(prompt, max_new_tokens=10, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "# t2 = time.time()\n",
    "# t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1767bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f434badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# examples = [\n",
    "#     \"How can I write a Python function to generate the nth Fibonacci number?\",\n",
    "#     \"How do I get the current date using shell commands? Explain how it works.\",\n",
    "#     \"What's the meaning of life?\",\n",
    "#     \"Write a function in Javascript to reverse words in a given string.\",\n",
    "#     \"Give the following data {'Name':['Tom', 'Brad', 'Kyle', 'Jerry'], 'Age':[20, 21, 19, 18], 'Height' : [6.1, 5.9, 6.0, 6.1]}. Can you plot one graph with two subplots as columns. The first is a bar graph showing the height of each person. The second is a bargraph showing the age of each person? Draw the graph in seaborn talk mode.\",\n",
    "#     \"Create a regex to extract dates from logs\",\n",
    "#     \"How to decode JSON into a typescript object\",\n",
    "#     \"Write a list into a jsonlines file and save locally\",\n",
    "# ]\n",
    "\n",
    "# def echo(message, history):\n",
    "    \n",
    "#     prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "#     prompt = prompt_template.format(query=message)\n",
    "#     # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "#     outputs = pipe(prompt, max_new_tokens=356, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "#     return str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "    \n",
    "# demo = gr.ChatInterface(fn=echo, examples=examples, title=\"StarChat Bot\")\n",
    "# #demo.launch(share=True)\n",
    "# demo.queue(concurrency_count=3).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d067b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f86073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1ce2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6395d958b2d84acd959399783bc3d0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c1c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc17593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "315.9149122238159"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "print(\"before model start\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ec8e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before model start 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "313.774373292923"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "print(\"before model start 2\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c56792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from refractml import *\n",
    "# from refractml.constants import MLModelFlavours\n",
    "\n",
    "# # new score functions\n",
    "# from mosaic_utils.ai.score.base import ScoreBase\n",
    "# from typing import Tuple, Union, List, Any\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5934ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScoreTemplateExample(ScoreBase):\n",
    "#     \"\"\"\n",
    "#     This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "#     \"\"\"    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.model_loaded = None\n",
    "        \n",
    "#         import torch\n",
    "#         from transformers import pipeline\n",
    "\n",
    "#         if self.model_loaded is None:\n",
    "#             print(\"LLM model loading from data section\")\n",
    "#             self.model_loaded = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "#     def request_processing_fn(self, request) :\n",
    "#         \"\"\"\n",
    "#         Processes Request Object -> List[Input data, ..]. It could be:\n",
    "#                A List Mapping of All Value Can Be one of : \n",
    "#                    - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "#                    - List[numpy.array(), numpy.array(), ...]\n",
    "#                    - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "#                    - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "#         :return: (n_inputs, payload's)\n",
    "        \n",
    "#         Warnings:\n",
    "#         1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "#            Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "#         \"\"\"\n",
    "#         final_payload = []\n",
    "#         raw_payload = request.json[\"payload\"]\n",
    "#         return (1, raw_payload) \n",
    "    \n",
    "#     def pre_processing_fn(self,payload):\n",
    "#         # All preprocessing step must occur in this section\n",
    "#         # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "#         # Not Doing Any Preprocessing Hence Returned payload\n",
    "#         print(\"payload is \", payload)\n",
    "        \n",
    "#         return payload\n",
    "\n",
    "#     def prediction_fn(self,\n",
    "#                       model: Any,\n",
    "#                       pre_processed_input \n",
    "#                       ):\n",
    "#         \"\"\"\n",
    "#                 Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "#                 :param model: Supported Model\n",
    "#                 :param pre_processed_input: Single Preprocessed Payload\n",
    "#                 :return: Prediction Value From the model\n",
    "                \n",
    "#                 Important Notes:\n",
    "#                 - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "#         \"\"\"\n",
    "#         model_loaded = self.model_loaded\n",
    "#         mod = model_loaded\n",
    "#         text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "        \n",
    "#         prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "#         prompt = prompt_template.format(query=text)\n",
    "#         # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "#         outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "#         preds = str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "        \n",
    "        \n",
    "#         print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "#         return preds\n",
    "\n",
    "#     class Meta:    \n",
    "#         # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "#         def __init__(self):\n",
    "#             self.name = \"Pre Hooked Me !\"\n",
    "#             self.pre_call_hooks.append(self.print_)\n",
    "#         def print_(self):\n",
    "#             print(self.name)\n",
    "#         pre_call_hooks = []\n",
    "#         post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6301f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScoreTemplateExample(ScoreBase):\n",
    "#     \"\"\"\n",
    "#     This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "#     \"\"\"    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         import pickle\n",
    "#         from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "#         self.model_loaded = None\n",
    "#         self.tokenizer = None\n",
    "#         if self.model_loaded is None:\n",
    "#             print(\"LLM model loading from data section\")\n",
    "#             self.model_loaded = T5ForConditionalGeneration.from_pretrained(\"/data/artifacts_gpu/model/\")\n",
    "\n",
    "#         if self.tokenizer is None:\n",
    "#             print(\"tokenizer object is loading from data section\")\n",
    "#             self.tokenizer = T5Tokenizer.from_pretrained(\"/data/artifacts_gpu/tokenizer/\")\n",
    "        \n",
    "    \n",
    "#     def request_processing_fn(self, request) :\n",
    "#         \"\"\"\n",
    "#         Processes Request Object -> List[Input data, ..]. It could be:\n",
    "#                A List Mapping of All Value Can Be one of : \n",
    "#                    - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "#                    - List[numpy.array(), numpy.array(), ...]\n",
    "#                    - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "#                    - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "#         :return: (n_inputs, payload's)\n",
    "        \n",
    "#         Warnings:\n",
    "#         1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "#            Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "#         \"\"\"\n",
    "#         final_payload = []\n",
    "#         raw_payload = request.json[\"payload\"]\n",
    "#         return (1, raw_payload) \n",
    "    \n",
    "#     def pre_processing_fn(self,payload):\n",
    "#         # All preprocessing step must occur in this section\n",
    "#         # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "#         # Not Doing Any Preprocessing Hence Returned payload\n",
    "#         print(\"payload is \", payload)\n",
    "        \n",
    "#         return payload\n",
    "\n",
    "#     def prediction_fn(self,\n",
    "#                       model: Any,\n",
    "#                       pre_processed_input \n",
    "#                       ):\n",
    "#         \"\"\"\n",
    "#                 Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "#                 :param model: Supported Model\n",
    "#                 :param pre_processed_input: Single Preprocessed Payload\n",
    "#                 :return: Prediction Value From the model\n",
    "                \n",
    "#                 Important Notes:\n",
    "#                 - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "#         \"\"\"\n",
    "#         model_loaded = self.model_loaded\n",
    "#         mod = model_loaded\n",
    "#         text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "#         tokenizer = self.tokenizer\n",
    "#         source = tokenizer.batch_encode_plus([text], max_length=512, pad_to_max_length=True,return_tensors='pt')\n",
    "#         source_ids = source['input_ids']\n",
    "#         source_mask = source['attention_mask']\n",
    "#         generated_ids = mod.generate(\n",
    "#             input_ids = source_ids,\n",
    "#             attention_mask = source_mask, \n",
    "#             max_length=150, \n",
    "#             num_beams=2,\n",
    "#             repetition_penalty=2.5, \n",
    "#             length_penalty=1.0, \n",
    "#             early_stopping=True\n",
    "\n",
    "#         )\n",
    "#         preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "#         print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "#         return preds\n",
    "\n",
    "#     class Meta:    \n",
    "#         # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "#         def __init__(self):\n",
    "#             self.name = \"Pre Hooked Me !\"\n",
    "#             self.pre_call_hooks.append(self.print_)\n",
    "#         def print_(self):\n",
    "#             print(self.name)\n",
    "#         pre_call_hooks = []\n",
    "#         post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34804572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: streamlit: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!streamlit run /opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb83123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10sec.py\r\n",
      " Dashboard_App2\r\n",
      " Falcon-7B.ipynb\r\n",
      " Flant5-xl-cpu.ipynb\r\n",
      " Flant5-xl-gpu.ipynb\r\n",
      "'GPU Utilisation Check.ipynb'\r\n",
      " Live_Usage_queries_with_mql_processed_rev3_curated.csv\r\n",
      " LlamaCoder.ipynb\r\n",
      " Microsoft-phi1.5.ipynb\r\n",
      " Octocoder.ipynb\r\n",
      " README.md\r\n",
      "'SCB StarChat Demo.ipynb'\r\n",
      "'SCB coder-GPU.ipynb'\r\n",
      "'SCB coder.ipynb'\r\n",
      " Startcoder-tiny.ipynb\r\n",
      " T5_sum_cpu.ipynb\r\n",
      " T5_sum_gpu.ipynb\r\n",
      " Untitled.ipynb\r\n",
      " Untitled1.ipynb\r\n",
      " Untitled2.ipynb\r\n",
      " Untitled3.ipynb\r\n",
      " __MACOSX\r\n",
      " __pycache__\r\n",
      " chatAppSCB5\r\n",
      " dash_gunicorn\r\n",
      " dialogues.py\r\n",
      " falcon_7b_summarization.ipynb\r\n",
      " gradio_app\r\n",
      " image_classification.ipynb\r\n",
      " llama-2-7b-fine-tuned-peft-v1\r\n",
      " llama-2-7b-fine-tuned-peft-v2\r\n",
      " llama2-fine-tuning-example.ipynb\r\n",
      "'llama2-fine-tuning-user-query-to-template-peft (1).ipynb'\r\n",
      " llama2-fine-tuning-user-query-to-template-peft.ipynb\r\n",
      " mistral-ft-code-generation-tryout.ipynb\r\n",
      " mistral-ft-user-query-to-template-peft-lora.ipynb\r\n",
      " predict_df1_0_100.csv\r\n",
      " predict_df_100_200.csv\r\n",
      " predict_df_200_300.csv\r\n",
      " predict_df_300_500.csv\r\n",
      " random.ipynb\r\n",
      " share_btn.py\r\n",
      " start_code\r\n",
      " streamlit\r\n",
      " streamlit_app\r\n",
      " streamlit_app_new\r\n",
      " template_and_user_query.csv\r\n",
      " template_and_user_query_v2.csv\r\n",
      " test_trainer\r\n",
      " thumbnail.png\r\n",
      " trainer_tf.ipynb\r\n",
      " trainer_torch.ipynb\r\n",
      " val_df.csv\r\n",
      " val_df_prediction.csv\r\n",
      " vit-base-beans\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8345054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nadded_tokens.json\\t\\t  \\nmodel-00004-of-00004.safetensors\\nconfig.json\\t\\t\\t  \\nmodel.safetensors.index.json\\ngeneration_config.json\\t\\t  \\nspecial_tokens_map.json\\nmerges.txt\\t\\t\\t  \\ntokenizer.json\\nmodel-00001-of-00004.safetensors  \\ntokenizer_config.json\\nmodel-00002-of-00004.safetensors  \\nvocab.json\\nmodel-00003-of-00004.safetensors\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "added_tokens.json\t\t  \n",
    "model-00004-of-00004.safetensors\n",
    "config.json\t\t\t  \n",
    "model.safetensors.index.json\n",
    "generation_config.json\t\t  \n",
    "special_tokens_map.json\n",
    "merges.txt\t\t\t  \n",
    "tokenizer.json\n",
    "model-00001-of-00004.safetensors  \n",
    "tokenizer_config.json\n",
    "model-00002-of-00004.safetensors  \n",
    "vocab.json\n",
    "model-00003-of-00004.safetensors\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc4e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31dcf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d4b791697645b3b65e096216a66746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df528d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.10747027397156"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8921019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!sudo huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2709e074",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "rahuldshetty/starchat-beta is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/rahuldshetty/starchat-beta/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[0;32m-> 1541\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6533d128-266d25c61e273d004817505b;095c2f21-ef8b-4c69-b3d2-683f4eafb82e)\n\nRepository Not Found for url: https://huggingface.co/rahuldshetty/starchat-beta/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrahuldshetty/starchat-beta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/pipelines/__init__.py:705\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 705\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    708\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:983\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    982\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 983\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    985\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:617\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    619\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:433\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: rahuldshetty/starchat-beta is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"rahuldshetty/starchat-beta\", device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "# You can sort a list in Python by using the sort() method. Here's an example:\\n\\n```\\nnumbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\\nnumbers.sort()\\nprint(numbers)\\n```\\n\\nThis will sort the list in place and print the sorted list.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
