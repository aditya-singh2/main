{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82315d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43271122cc544b66b2e096a1fa1b87ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at HuggingFaceH4/starchat-beta and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cfbbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130.81290984153748"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb81f5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|system|>',\n",
       " '<|end|>',\n",
       " '<|user|>',\n",
       " 'How do I sort a list in Python?<|end|>',\n",
       " '<|assistant|>',\n",
       " 'There are several ways to sort a list in Python. The most common way is to use the sort() method.',\n",
       " '',\n",
       " 'Here is an example:',\n",
       " '',\n",
       " '```',\n",
       " 'numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]',\n",
       " 'numbers.sort()',\n",
       " 'print(numbers)',\n",
       " 'shuffle(numbers)',\n",
       " '```',\n",
       " '',\n",
       " 'This will sort the list in place and print the sorted list.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][\"generated_text\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4432b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.593522787094116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=10, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1767bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting gradio\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8f0c3b640>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/05/f2/360ca9546cffa45fee1df56864fdc2b6955de622e98435539490cd882a96/gradio-3.47.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/f2/360ca9546cffa45fee1df56864fdc2b6955de622e98435539490cd882a96/gradio-3.47.1-py3-none-any.whl (20.3MB)\n",
      "\u001b[K     |████████████████████████████████| 20.3MB 7.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml<7.0,>=5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/6b/6600ac24725c7388255b2f5add93f91e58a5d7efaf4af244fdbcc11a541b/PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736kB)\n",
      "\u001b[K     |████████████████████████████████| 737kB 78.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gradio-client==0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/dd/db8dcc8521aa475a18a562929de0806819f8fa73ee8654d2cc22c836c3bd/gradio_client-0.6.0-py3-none-any.whl (298kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 108.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests~=2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 101.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting altair<6.0,>=4.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/16/b12fca347ff9d062e3c44ad9641d2ec50364570a059f3078ada3a5119d7a/altair-5.1.2-py3-none-any.whl (516kB)\n",
      "\u001b[K     |████████████████████████████████| 522kB 83.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow<11.0,>=8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/ae/4050c0d3ca5d3106cbbce376c58c061dce0a6ff77dae6a30f8e3d285ee7c/Pillow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5MB 125.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting uvicorn>=0.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 101.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib~=3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/73/a4af3493a81d6e5e1fdb4c72f4d3573a7e94b60f7c2c69ab0275fdc7cd8e/matplotlib-3.7.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2MB 109.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiofiles<24.0,>=22.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl\n",
      "Collecting packaging\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 92.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas<3.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/7f/5b047effafbdd34e52c9e2d7e44f729a0655efafb22198c45cf692cdc157/pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4MB 106.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydub\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl\n",
      "Collecting python-multipart\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/ff/b1e11d8bffb5e0e1b6d27f402eeedbeb9be6df2cdbc09356a1ae49806dbf/python_multipart-0.0.6-py3-none-any.whl (45kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 87.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2<4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/c3/f068337a370801f372f2f8f6bad74a5c140f6fda3d9de154052708dd3c65/Jinja2-3.1.2-py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 121.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastapi\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/d2/3ad038a2365fefbac19d9a046cab7ce45f4c7bfa81d877cbece9707de9ce/fastapi-0.103.2-py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 98.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/66/0a72c9fcde42e5650c8d8d5c5c1873b9a3893018020c77ca8eb62708b923/pydantic-2.4.2-py3-none-any.whl (395kB)\n",
      "\u001b[K     |████████████████████████████████| 399kB 130.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 111.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 104.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting semantic-version~=2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/23/8146aad7d88f4fcb3a6218f41a60f6c2d4e3a72de72da1825dc7c8f7877c/semantic_version-2.10.0-py2.py3-none-any.whl\n",
      "Collecting typing-extensions~=4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl\n",
      "Collecting markupsafe~=2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/de/e2/32c14301bb023986dff527a49325b6259cab4ebb4633f69de54af312fc45/MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting importlib-resources<7.0,>=1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/65/6e/09d8816b5cb7a4006ef8ad1717a2703ad9f331dae9717d9f22488a2d6469/importlib_resources-6.1.0-py3-none-any.whl\n",
      "Collecting websockets<12.0,>=10.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/05/2efb520317340ece74bfc4d88e8f011dd71a4e6c263000bfffb71a343685/websockets-11.0.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 111.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5d/5738903efe0ecb73e51eb44feafba32bdba2081263d40c5043568ff60faf/numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 105.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/06/49b275a312eb207e2a2718a7414dedfded05088437352b67aaa9a355f948/ffmpy-0.3.1.tar.gz\n",
      "Collecting orjson~=3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/45/c9168772b1d1360525287df9e7d3e807f636841f681787e25f7cf3b36a5c/orjson-3.9.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 94.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl (173kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 115.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 104.1MB/s eta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/40/9957270221b6d3e9a3b92fdfba80dd5c9661ff45a664b47edd5d00f707f5/urllib3-2.0.6-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 105.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/34/3030de6f1370931b9dbb4dad48f6ab1015ab1d32447850b9fc94e60097be/idna-3.4-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 108.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c8/fd52271326c052f95f47ef718b018aa2bc3fd097d9bac44d7d48894c6130/charset_normalizer-3.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 126.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toolz\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/5c/922a3508f5bda2892be3df86c74f9cf1e01217c2b1f8a0ac4841d903e3e9/toolz-0.12.0-py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 94.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema>=3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/bf/a84bc75f069f4f156e1c0d9892fb7325945106c6ecaad9f29d24360872af/jsonschema-4.19.1-py3-none-any.whl (83kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 103.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click>=7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 106.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting h11>=0.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 93.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/71/7f20855592cc929bc206810432b991ec4c702dc26b0567b132e52c85536f/contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 98.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/55/7021ffcc8cb26a520bb051aa0a3d08daf200cde945e5863d5768161e2d3d/kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 111.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl (247kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 108.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 116.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl\n",
      "Collecting fonttools>=4.22.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b8/965acdc0510d0ceca51bb9b9286a600eb8221d1e79d29da16f5bf035deab/fonttools-4.43.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6MB 99.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl (502kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 84.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/fb/a79efcab32b8a1f1ddca7f35109a50e4a80d42ac1c9187ab46522b2407d7/tzdata-2023.3-py2.py3-none-any.whl (341kB)\n",
      "\u001b[K     |████████████████████████████████| 348kB 110.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 74.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting anyio<4.0.0,>=3.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 98.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic-core==2.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/82/4139ea7aebec9e22be42d8a5342df5022b7b6b6deb10126dc3ff80b4d6fd/pydantic_core-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 97.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl\n",
      "Collecting tqdm>=4.42.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 87.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/5d/97afbafd9d584ff1b45fcb354a479a3609bd97f912f8f1f6c563cb1fae21/filelock-3.12.4-py3-none-any.whl\n",
      "Collecting sniffio\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/a0/5dba8ed157b0136607c7f2151db695885606968d1fae123dc3391e0cfdbf/sniffio-1.3.0-py3-none-any.whl\n",
      "Collecting httpcore<0.19.0,>=0.18.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 99.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=3.1.0; python_version < \"3.10\"\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/24/83349ac2189cc2435e84da3f69ba3c97314d3c0622628e55171c6798ed80/jsonschema_specifications-2023.7.1-py3-none-any.whl\n",
      "Collecting attrs>=22.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/eb/fcb708c7bf5056045e9e98f62b93bd7467eb718b0202e7698eb11d66416c/attrs-23.1.0-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 87.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rpds-py>=0.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/a1/3c46b6593e54982779e1d96b53ad4db275d8ca4bb43dae37d547e56f64e0/rpds_py-0.10.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 101.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting referencing>=0.28.4\n",
      "  Downloading https://files.pythonhosted.org/packages/be/8e/56d6f1e2d591f4d6cbcba446cac4a1b0dc4f584537e2071d9bcee8eeab6b/referencing-0.30.2-py3-none-any.whl\n",
      "Collecting pkgutil-resolve-name>=1.3.10; python_version < \"3.9\"\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/5c/3d4882ba113fd55bdba9326c1e4c62a15e674a2501de4869e6bd6301f87e/pkgutil_resolve_name-1.3.10-py3-none-any.whl\n",
      "Collecting six>=1.5\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting exceptiongroup; python_version < \"3.11\"\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/83/b71e58666f156a39fb29417e4c8ca4bc7400c0dd4ed9e8842ab54dc8c344/exceptiongroup-1.1.3-py3-none-any.whl\n",
      "Building wheels for collected packages: ffmpy\n",
      "\u001b[33m  WARNING: Building wheel for ffmpy failed: [Errno 13] Permission denied: '/home/mosaic-ai/.cache/pip/wheels/f2'\u001b[0m\n",
      "Failed to build ffmpy\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement PyYAML==6.0, but you'll have pyyaml 6.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement urllib3==1.26.15, but you'll have urllib3 2.0.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.0.0a1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-auth 2.22.0 has requirement urllib3<2.0, but you'll have urllib3 2.0.6 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyyaml, typing-extensions, fsspec, websockets, packaging, certifi, urllib3, idna, charset-normalizer, requests, tqdm, filelock, huggingface-hub, sniffio, h11, exceptiongroup, anyio, httpcore, httpx, gradio-client, toolz, zipp, importlib-resources, rpds-py, attrs, referencing, jsonschema-specifications, pkgutil-resolve-name, jsonschema, numpy, pytz, six, python-dateutil, tzdata, pandas, markupsafe, jinja2, altair, pillow, click, uvicorn, contourpy, kiwisolver, pyparsing, cycler, fonttools, matplotlib, aiofiles, pydub, python-multipart, starlette, pydantic-core, annotated-types, pydantic, fastapi, semantic-version, ffmpy, orjson, gradio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Running setup.py install for ffmpy ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed aiofiles-23.2.1 altair-5.1.2 annotated-types-0.6.0 anyio-3.7.1 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.0 click-8.1.7 contourpy-1.1.1 cycler-0.12.1 exceptiongroup-1.1.3 fastapi-0.103.2 ffmpy-0.3.1 filelock-3.12.4 fonttools-4.43.1 fsspec-2023.9.2 gradio-3.47.1 gradio-client-0.6.0 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 idna-3.4 importlib-resources-6.1.0 jinja2-3.1.2 jsonschema-4.19.1 jsonschema-specifications-2023.7.1 kiwisolver-1.4.5 markupsafe-2.1.3 matplotlib-3.7.3 numpy-1.24.4 orjson-3.9.9 packaging-23.2 pandas-2.0.3 pillow-10.0.1 pkgutil-resolve-name-1.3.10 pydantic-2.4.2 pydantic-core-2.10.1 pydub-0.25.1 pyparsing-3.1.1 python-dateutil-2.8.2 python-multipart-0.0.6 pytz-2023.3.post1 pyyaml-6.0.1 referencing-0.30.2 requests-2.31.0 rpds-py-0.10.6 semantic-version-2.10.0 six-1.16.0 sniffio-1.3.0 starlette-0.27.0 toolz-0.12.0 tqdm-4.66.1 typing-extensions-4.8.0 tzdata-2023.3 urllib3-2.0.6 uvicorn-0.23.2 websockets-11.0.3 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f434badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-y67o66nx because the default path (/home/mosaic-ai/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a7b8e7f84940bcea4f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a7b8e7f84940bcea4f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "examples = [\n",
    "    \"How can I write a Python function to generate the nth Fibonacci number?\",\n",
    "    \"How do I get the current date using shell commands? Explain how it works.\",\n",
    "    \"What's the meaning of life?\",\n",
    "    \"Write a function in Javascript to reverse words in a given string.\",\n",
    "    \"Give the following data {'Name':['Tom', 'Brad', 'Kyle', 'Jerry'], 'Age':[20, 21, 19, 18], 'Height' : [6.1, 5.9, 6.0, 6.1]}. Can you plot one graph with two subplots as columns. The first is a bar graph showing the height of each person. The second is a bargraph showing the age of each person? Draw the graph in seaborn talk mode.\",\n",
    "    \"Create a regex to extract dates from logs\",\n",
    "    \"How to decode JSON into a typescript object\",\n",
    "    \"Write a list into a jsonlines file and save locally\",\n",
    "]\n",
    "\n",
    "def echo(message, history):\n",
    "    \n",
    "    prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "    prompt = prompt_template.format(query=message)\n",
    "    # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "    outputs = pipe(prompt, max_new_tokens=356, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "    return str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "    \n",
    "demo = gr.ChatInterface(fn=echo, examples=examples, title=\"StarChat Bot\")\n",
    "#demo.launch(share=True)\n",
    "demo.queue(concurrency_count=3).launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d067b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/refs/main\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f86073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--CodeFuse-13B\r\n",
      "models--HuggingFaceH4--starchat-beta\r\n",
      "models--Salesforce--codegen-350M-mono\r\n",
      "models--TheBloke--Redmond-Hermes-Coder-GGML\r\n",
      "models--TheBloke--Redmond-Hermes-Coder-GPTQ\r\n",
      "models--TheBloke--starcoderplus-GPTQ\r\n",
      "models--bigcode--starcoderbase-1b\r\n",
      "models--bigcode--starcoderplus\r\n",
      "models--bigcode--tiny_starcoder_py\r\n",
      "models--microsoft--phi-1_5\r\n",
      "models--mistralai--Mistral-7B-Instruct-v0.1\r\n",
      "tmp9zao9gd7\r\n",
      "tmpjox9s4fp\r\n",
      "version.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/huggingface/cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe1ce2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7c74adfaa84d5cb7039038197bcb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTBigCodeForCausalLM were not initialized from the model checkpoint at /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "#pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/starchat-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "pipe = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c1c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f7c8fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:49155 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.10395216941833"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a variant of ChatML to format each message\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "prompt = prompt_template.format(query=\"How do I sort a list in Python?\")\n",
    "# We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc17593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json\t\t  model-00004-of-00004.safetensors\r\n",
      "config.json\t\t\t  model.safetensors.index.json\r\n",
      "generation_config.json\t\t  special_tokens_map.json\r\n",
      "merges.txt\t\t\t  tokenizer.json\r\n",
      "model-00001-of-00004.safetensors  tokenizer_config.json\r\n",
      "model-00002-of-00004.safetensors  vocab.json\r\n",
      "model-00003-of-00004.safetensors\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ec8e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/notebooks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c56792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refractml import *\n",
    "from refractml.constants import MLModelFlavours\n",
    "\n",
    "# new score functions\n",
    "from mosaic_utils.ai.score.base import ScoreBase\n",
    "from typing import Tuple, Union, List, Any\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5934ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_loaded = None\n",
    "        \n",
    "        import torch\n",
    "        from transformers import pipeline\n",
    "\n",
    "        if self.model_loaded is None:\n",
    "            print(\"LLM model loading from data section\")\n",
    "            self.model_loaded = pipeline(\"text-generation\", model=\"/data/huggingface/cache/models--HuggingFaceH4--starchat-beta/snapshots/b1bcda690655777373f57ea6614eb095ec2c886f\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "    def request_processing_fn(self, request) :\n",
    "        \"\"\"\n",
    "        Processes Request Object -> List[Input data, ..]. It could be:\n",
    "               A List Mapping of All Value Can Be one of : \n",
    "                   - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "                   - List[numpy.array(), numpy.array(), ...]\n",
    "                   - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "                   - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "        :return: (n_inputs, payload's)\n",
    "        \n",
    "        Warnings:\n",
    "        1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "           Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "        \"\"\"\n",
    "        final_payload = []\n",
    "        raw_payload = request.json[\"payload\"]\n",
    "        return (1, raw_payload) \n",
    "    \n",
    "    def pre_processing_fn(self,payload):\n",
    "        # All preprocessing step must occur in this section\n",
    "        # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "        # Not Doing Any Preprocessing Hence Returned payload\n",
    "        print(\"payload is \", payload)\n",
    "        \n",
    "        return payload\n",
    "\n",
    "    def prediction_fn(self,\n",
    "                      model: Any,\n",
    "                      pre_processed_input \n",
    "                      ):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "                \n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "        \"\"\"\n",
    "        model_loaded = self.model_loaded\n",
    "        mod = model_loaded\n",
    "        text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "        \n",
    "        prompt_template = \"<|system|>\\n<|end|>\\n<|user|>\\n{query}<|end|>\\n<|assistant|>\"\n",
    "        prompt = prompt_template.format(query=text)\n",
    "        # We use a special <|end|> token with ID 49155 to denote ends of a turn\n",
    "        outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)\n",
    "        preds = str(outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1])\n",
    "        \n",
    "        \n",
    "        print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "        return preds\n",
    "\n",
    "    class Meta:    \n",
    "        # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "        def __init__(self):\n",
    "            self.name = \"Pre Hooked Me !\"\n",
    "            self.pre_call_hooks.append(self.print_)\n",
    "        def print_(self):\n",
    "            print(self.name)\n",
    "        pre_call_hooks = []\n",
    "        post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6301f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreTemplateExample(ScoreBase):\n",
    "    \"\"\"\n",
    "    This Class Demonstrate How To Implements ScoreBase Interface Class And It Basic Usage.\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        import pickle\n",
    "        from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "        self.model_loaded = None\n",
    "        self.tokenizer = None\n",
    "        if self.model_loaded is None:\n",
    "            print(\"LLM model loading from data section\")\n",
    "            self.model_loaded = T5ForConditionalGeneration.from_pretrained(\"/data/artifacts_gpu/model/\")\n",
    "\n",
    "        if self.tokenizer is None:\n",
    "            print(\"tokenizer object is loading from data section\")\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(\"/data/artifacts_gpu/tokenizer/\")\n",
    "        \n",
    "    \n",
    "    def request_processing_fn(self, request) :\n",
    "        \"\"\"\n",
    "        Processes Request Object -> List[Input data, ..]. It could be:\n",
    "               A List Mapping of All Value Can Be one of : \n",
    "                   - List[ [Feature_Value1, Feature_Value2, ...], [...] ]\n",
    "                   - List[numpy.array(), numpy.array(), ...]\n",
    "                   - List[tf.Tensor, tf.Tensor, tf.Tensor, ...]\n",
    "                   - List[ SingleSample, SingleSample]\n",
    "                   \n",
    "        :return: (n_inputs, payload's)\n",
    "        \n",
    "        Warnings:\n",
    "        1. Do not reshape your final output for single sample here, do it in prediction.\n",
    "           Else payloads will be invalidated for extraction at raw and extraction level.\n",
    "        \"\"\"\n",
    "        final_payload = []\n",
    "        raw_payload = request.json[\"payload\"]\n",
    "        return (1, raw_payload) \n",
    "    \n",
    "    def pre_processing_fn(self,payload):\n",
    "        # All preprocessing step must occur in this section\n",
    "        # Takes Single Sample -> Returns Single Sample\n",
    "        \n",
    "        # Not Doing Any Preprocessing Hence Returned payload\n",
    "        print(\"payload is \", payload)\n",
    "        \n",
    "        return payload\n",
    "\n",
    "    def prediction_fn(self,\n",
    "                      model: Any,\n",
    "                      pre_processed_input \n",
    "                      ):\n",
    "        \"\"\"\n",
    "                Does the main prediction on pre_processed_input(Single Sample) using supplied model .\n",
    "\n",
    "                :param model: Supported Model\n",
    "                :param pre_processed_input: Single Preprocessed Payload\n",
    "                :return: Prediction Value From the model\n",
    "                \n",
    "                Important Notes:\n",
    "                - Reshape your data array.reshape(1, -1) before predictions as it contains a single sample.\n",
    "                    \n",
    "        \"\"\"\n",
    "        model_loaded = self.model_loaded\n",
    "        mod = model_loaded\n",
    "        text = pre_processed_input #this is tuple we can iterate if there is number of input\n",
    "        tokenizer = self.tokenizer\n",
    "        source = tokenizer.batch_encode_plus([text], max_length=512, pad_to_max_length=True,return_tensors='pt')\n",
    "        source_ids = source['input_ids']\n",
    "        source_mask = source['attention_mask']\n",
    "        generated_ids = mod.generate(\n",
    "            input_ids = source_ids,\n",
    "            attention_mask = source_mask, \n",
    "            max_length=150, \n",
    "            num_beams=2,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True\n",
    "\n",
    "        )\n",
    "        preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "        print(\"prediction is \\n: \",preds)\n",
    "    \n",
    "        return preds\n",
    "\n",
    "    class Meta:    \n",
    "        # List of Callables() can be attached For Calling After AnSd Before Scoring\n",
    "        def __init__(self):\n",
    "            self.name = \"Pre Hooked Me !\"\n",
    "            self.pre_call_hooks.append(self.print_)\n",
    "        def print_(self):\n",
    "            print(self.name)\n",
    "        pre_call_hooks = []\n",
    "        post_call_hooks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949418a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c0257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting streamlit\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/01/9d1460a6e71d6226e1f5c5053de20758c41100c5c15abb73551273a96fb8/streamlit-1.27.2-py2.py3-none-any.whl (7.6MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6MB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting altair<6,>=4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/16/b12fca347ff9d062e3c44ad9641d2ec50364570a059f3078ada3a5119d7a/altair-5.1.2-py3-none-any.whl (516kB)\n",
      "\u001b[K     |████████████████████████████████| 522kB 110.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil<3,>=2.7.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl (247kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 107.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzlocal<6,>=1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/af/343114b3ed9500b46108b56569b31a3108d3669d4fd063d9640e2c36cd57/tzlocal-5.1-py3-none-any.whl\n",
      "Collecting typing-extensions<5,>=4.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/7e/20f7e45878b5aed34320fbeeae8f78acc806e7bd708d00b1c6e64b016f5b/GitPython-3.1.37-py3-none-any.whl (190kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 111.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tornado<7,>=6.0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/a5/e6da56c03ff61200d5a43cfb75ab09316fc0836aa7ee26b4e9dcbfc3ae85/tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427kB)\n",
      "\u001b[K     |████████████████████████████████| 430kB 127.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting watchdog>=2.1.5; platform_system != \"Darwin\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/f0/456948b865ab259784f774154e7d65844fa9757522fdb11533fbf8ae7aca/watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 128.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting validators<1,>=0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl\n",
      "Collecting numpy<2,>=1.19.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5d/5738903efe0ecb73e51eb44feafba32bdba2081263d40c5043568ff60faf/numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 108.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging<24,>=16.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 99.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf<5,>=3.20\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/2c/03046cac73f46bfe98fc846ef629cf4f84c2f59258216aa2cc0d22bfca8f/protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 108.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6,>=4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl\n",
      "Collecting rich<14,>=10.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/2a/4e62ff633612f746f88618852a626bbe24226eba5e7ac90e91dcfd6a414e/rich-13.6.0-py3-none-any.whl (239kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 116.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow<11,>=7.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/ae/4050c0d3ca5d3106cbbce376c58c061dce0a6ff77dae6a30f8e3d285ee7c/Pillow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5MB 108.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity<9,>=8.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/2b/11bcedb7dee4923253a4a21bae3be854bcc4f06295bd827756352016d97c/blinker-1.6.3-py3-none-any.whl\n",
      "Collecting requests<3,>=2.27\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 102.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pydeck<1,>=0.8.0b4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/4b/2fc80540e2d3903452245bb657c7f758ec7342420507d1e4091b0161856e/pydeck-0.8.1b0-py2.py3-none-any.whl (4.8MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8MB 104.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas<3,>=1.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/7f/5b047effafbdd34e52c9e2d7e44f729a0655efafb22198c45cf692cdc157/pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4MB 112.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toml<2,>=0.10.1\n",
      "  Downloading https://files.pythonhosted.org/packages/44/6f/7120676b6d73228c96e17f1f794d8ab046fc910d781c8d151120c3f1569e/toml-0.10.2-py2.py3-none-any.whl\n",
      "Collecting importlib-metadata<7,>=1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/37/db7ba97e676af155f5fcb1a35466f446eadc9104e25b83366e8088c9c926/importlib_metadata-6.8.0-py3-none-any.whl\n",
      "Collecting click<9,>=7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 99.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/4f/1e721c6cee69b62251c996266b0cf256faad38028cb3b6d71cc86a86109e/pyarrow-13.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.1MB)\n",
      "\u001b[K     |████████████████████████████████| 40.2MB 107.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/c3/f068337a370801f372f2f8f6bad74a5c140f6fda3d9de154052708dd3c65/Jinja2-3.1.2-py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 109.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toolz\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/5c/922a3508f5bda2892be3df86c74f9cf1e01217c2b1f8a0ac4841d903e3e9/toolz-0.12.0-py3-none-any.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 86.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema>=3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/bf/a84bc75f069f4f156e1c0d9892fb7325945106c6ecaad9f29d24360872af/jsonschema-4.19.1-py3-none-any.whl (83kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 105.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.5\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting backports.zoneinfo; python_version < \"3.9\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/ab/3e941e3fcf1b7d3ab3d0233194d99d6a0ed6b24f8f956fc81e47edc8c079/backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 87.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/a6/35f83efec687615c711fe0a09b67e58f6d1254db27b1013119de46f450bd/gitdb-4.0.10-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 106.6MB/s ta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl (87kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 111.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/88/29adf0b44ba6ac85045e63734ae0997d3c58d8b1a91c914d240828d0d73d/Pygments-2.16.1-py3-none-any.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 114.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/40/9957270221b6d3e9a3b92fdfba80dd5c9661ff45a664b47edd5d00f707f5/urllib3-2.0.6-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 125.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 113.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/34/3030de6f1370931b9dbb4dad48f6ab1015ab1d32447850b9fc94e60097be/idna-3.4-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 100.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c8/fd52271326c052f95f47ef718b018aa2bc3fd097d9bac44d7d48894c6130/charset_normalizer-3.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 114.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/fb/a79efcab32b8a1f1ddca7f35109a50e4a80d42ac1c9187ab46522b2407d7/tzdata-2023.3-py2.py3-none-any.whl (341kB)\n",
      "\u001b[K     |████████████████████████████████| 348kB 113.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl (502kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 110.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/de/e2/32c14301bb023986dff527a49325b6259cab4ebb4633f69de54af312fc45/MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading https://files.pythonhosted.org/packages/be/8e/56d6f1e2d591f4d6cbcba446cac4a1b0dc4f584537e2071d9bcee8eeab6b/referencing-0.30.2-py3-none-any.whl\n",
      "Collecting rpds-py>=0.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/a1/3c46b6593e54982779e1d96b53ad4db275d8ca4bb43dae37d547e56f64e0/rpds_py-0.10.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 102.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources>=1.4.0; python_version < \"3.9\"\n",
      "  Downloading https://files.pythonhosted.org/packages/65/6e/09d8816b5cb7a4006ef8ad1717a2703ad9f331dae9717d9f22488a2d6469/importlib_resources-6.1.0-py3-none-any.whl\n",
      "Collecting pkgutil-resolve-name>=1.3.10; python_version < \"3.9\"\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/5c/3d4882ba113fd55bdba9326c1e4c62a15e674a2501de4869e6bd6301f87e/pkgutil_resolve_name-1.3.10-py3-none-any.whl\n",
      "Collecting attrs>=22.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/eb/fcb708c7bf5056045e9e98f62b93bd7467eb718b0202e7698eb11d66416c/attrs-23.1.0-py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 79.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/24/83349ac2189cc2435e84da3f69ba3c97314d3c0622628e55171c6798ed80/jsonschema_specifications-2023.7.1-py3-none-any.whl\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement urllib3==1.26.15, but you'll have urllib3 2.0.6 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.0.0a1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-auth 2.22.0 has requirement urllib3<2.0, but you'll have urllib3 2.0.6 which is incompatible.\u001b[0m\n",
      "Installing collected packages: MarkupSafe, jinja2, packaging, toolz, typing-extensions, numpy, six, python-dateutil, tzdata, pytz, pandas, rpds-py, attrs, referencing, zipp, importlib-resources, pkgutil-resolve-name, jsonschema-specifications, jsonschema, altair, backports.zoneinfo, tzlocal, smmap, gitdb, gitpython, tornado, watchdog, validators, protobuf, cachetools, mdurl, markdown-it-py, pygments, rich, pillow, tenacity, blinker, urllib3, certifi, idna, charset-normalizer, requests, pydeck, toml, importlib-metadata, click, pyarrow, streamlit\n",
      "Successfully installed MarkupSafe-2.1.3 altair-5.1.2 attrs-23.1.0 backports.zoneinfo-0.2.1 blinker-1.6.3 cachetools-5.3.1 certifi-2023.7.22 charset-normalizer-3.3.0 click-8.1.7 gitdb-4.0.10 gitpython-3.1.37 idna-3.4 importlib-metadata-6.8.0 importlib-resources-6.1.0 jinja2-3.1.2 jsonschema-4.19.1 jsonschema-specifications-2023.7.1 markdown-it-py-3.0.0 mdurl-0.1.2 numpy-1.24.4 packaging-23.2 pandas-2.0.3 pillow-10.0.1 pkgutil-resolve-name-1.3.10 protobuf-4.24.4 pyarrow-13.0.0 pydeck-0.8.1b0 pygments-2.16.1 python-dateutil-2.8.2 pytz-2023.3.post1 referencing-0.30.2 requests-2.31.0 rich-13.6.0 rpds-py-0.10.6 six-1.16.0 smmap-5.0.1 streamlit-1.27.2 tenacity-8.2.3 toml-0.10.2 toolz-0.12.0 tornado-6.3.3 typing-extensions-4.8.0 tzdata-2023.3 tzlocal-5.1 urllib3-2.0.6 validators-0.22.0 watchdog-3.0.0 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54883715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 13:48:52.151 WARNING streamlit.config: \n",
      "Warning: the config option 'server.enableCORS=false' is not compatible with 'server.enableXsrfProtection=true'.\n",
      "As a result, 'server.enableCORS' is being overridden to 'true'.\n",
      "\n",
      "More information:\n",
      "In order to protect against CSRF attacks, we send a cookie with each request.\n",
      "To do so, we must specify allowable origins, which places a restriction on\n",
      "cross-origin resource sharing.\n",
      "\n",
      "If cross origin resource sharing is required, please disable server.enableXsrfProtection.\n",
      "            \n",
      "2023-10-12 13:48:52.153 WARNING streamlit: \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "#import streamlit.components.v1 as components\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "with st.chat_message(\"user\"):\n",
    "    st.write(\"Hello 👋\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34804572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: streamlit: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!streamlit run /opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb83123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
